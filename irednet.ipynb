{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_VISIBLE_DEVICES=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LZDR_UnY2aO",
        "outputId": "a3643b67-ab19-4096-cb74-efea751715a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-image==0.17.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj6Sq0SnfoDo",
        "outputId": "4cf24fcf-492f-4fae-9a90-b681554332e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-image==0.17.2\n",
            "  Downloading scikit-image-0.17.2.tar.gz (29.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.8/29.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.17.2) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.17.2) (1.11.4)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.17.2) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.17.2) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.17.2) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.17.2) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.17.2) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.17.2) (1.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (1.16.0)\n",
            "Building wheels for collected packages: scikit-image\n",
            "  Building wheel for scikit-image (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-image: filename=scikit_image-0.17.2-cp310-cp310-linux_x86_64.whl size=33208199 sha256=619daa77c596f055b1bea8a0bb1926af97cde6e01b2e97de452edd5a82705485\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/fc/f4/10c6987c3a9cb937913fa19f8290e91e6f66df2440c2af6130\n",
            "Successfully built scikit-image\n",
            "Installing collected packages: scikit-image\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.19.3\n",
            "    Uninstalling scikit-image-0.19.3:\n",
            "      Successfully uninstalled scikit-image-0.19.3\n",
            "Successfully installed scikit-image-0.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config for training"
      ],
      "metadata": {
        "id": "y3jDdmvDz4Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Ablation study train examples\n",
        "### Loss function\n",
        "# python train.py --network IReDNet --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_NegativeSSIM --data_path datasets/train/RainTrainL --batch_size 2 --epochs 100 --milestone 30 50 80 --optimizer Adam\n",
        "# python train.py --network IReDNet --loss SSIM --save_path logs/Rain100L/IReDNet_SSIM --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network IReDNet --loss MSE --save_path logs/Rain100L/IReDNet_MSE --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "\n",
        "# python train.py --network LightIReDNet --loss NegativeSSIM --save_path logs/Rain100L/LightIReDNet_NegativeSSIM --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network LightIReDNet --loss SSIM --save_path logs/Rain100L/LightIReDNet_SSIM --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network LightIReDNet --loss MSE --save_path logs/Rain100L/LightIReDNet_MSE --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "\n",
        "### Network architecture\n",
        "## Recurrent Layers Analysis\n",
        "# python train.py --network IteDNet --loss NegativeSSIM --save_path logs/Rain100L/IteDNet --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network IReDNet_LSTM --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_LSTM --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network IReDNet_GRU --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_GRU --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network IReDNet_BiRNN --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_BiRNN --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network IReDNet_IndRNN --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_IndRNN --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network IReDNet_ConvLSTM --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_ConvLSTM --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network IReDNet_QRNNs --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_QRNNs --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network IReDNet --loss NegativeSSIM --save_path logs/Rain100L/IReDNet --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "\n",
        "# python train.py --network LightIteDNet --loss NegativeSSIM --save_path logs/Rain100L/LightIteDNet --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network LightIReDNet_LSTM --loss NegativeSSIM --save_path logs/Rain100L/LightIReDNet_LSTM --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network LightIReDNet_GRU --loss NegativeSSIM --save_path logs/Rain100L/LightIReDNet_GRU --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network LightIReDNet_BiRNN --loss NegativeSSIM --save_path logs/Rain100L/LightIReDNet_BiRNN --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network LightIReDNet_IndRNN --loss NegativeSSIM --save_path logs/Rain100L/LightIReDNet_IndRNN --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network LightIReDNet_ConvLSTM --loss NegativeSSIM --save_path logs/Rain100L/LightIReDNet_ConvLSTM --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network LightIReDNet_QRNN --loss NegativeSSIM --save_path logs/Rain100L/LightIReDNet_QRNN --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --network LightIReDNet --loss NegativeSSIM --save_path logs/Rain100L/LightIReDNet --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "\n",
        "\n",
        "## Effects of recursive stage numbers\n",
        "# python train.py --recurrent_iter 2 --network IReDNet --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_2 --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --recurrent_iter 3 --network IReDNet --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_3 --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --recurrent_iter 4 --network IReDNet --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_4 --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --recurrent_iter 5 --network IReDNet --loss NegativeSSIM --save_path logs/Rain100L/IReDNet_5 --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "\n",
        "# python train.py --recurrent_iter 2 --network LightIteDNet --loss NegativeSSIM --save_path logs/Rain100L/LightIteDNet_2 --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --recurrent_iter 3 --network LightIteDNet --loss NegativeSSIM --save_path logs/Rain100L/LightIteDNet_3 --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --recurrent_iter 4 --network LightIteDNet --loss NegativeSSIM --save_path logs/Rain100L/LightIteDNet_4 --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n",
        "# python train.py --recurrent_iter 5 --network LightIteDNet --loss NegativeSSIM --save_path logs/Rain100L/LightIteDNet_5 --data_path datasets/train/RainTrainL --batch_size 4 --epochs 25 --milestone 8 16 20 --optimizer Adam\n"
      ],
      "metadata": {
        "id": "P3mJ4V2D8UwV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the parameters\n",
        "network = \"IReDNet\"\n",
        "loss = \"NegativeSSIM\"\n",
        "optimizer = \"Adam\"\n",
        "use_gpu = True\n",
        "batch_size = 2\n",
        "epochs = 100\n",
        "milestone = [30, 50, 80]\n",
        "save_path = \"/content/logs/Rain100L/IReDNet_NegativeSSIM\"\n",
        "data_path = \"/content/datasets/train/RainTrainL\"\n",
        "preprocess = True\n",
        "lr = 1e-3\n",
        "save_freq = 1\n",
        "gpu_id = \"0\"\n",
        "recurrent_iter = 6"
      ],
      "metadata": {
        "id": "ME8g6hPvz6Du"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config for testing"
      ],
      "metadata": {
        "id": "1h5vKjzdz1jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = \"IReDNet_BiRNN\"\n",
        "logdir = \"/content/logs/Rain100L/IReDNet_BiRNN/\"\n",
        "data_path = \"/content/datasets/test/Rain100L/rainy\"\n",
        "save_path = \"/content/results/Rain100L/IReDNet_BiRNN\"\n",
        "use_gpu = True\n",
        "gpu_id = \"0\"\n",
        "recurrent_iter = 6"
      ],
      "metadata": {
        "id": "VHS162U-i34t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import re\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from skimage.measure.simple_metrics import compare_psnr\n",
        "import  os\n",
        "import glob\n",
        "\n",
        "\n",
        "def findLastCheckpoint(save_dir):\n",
        "    file_list = glob.glob(os.path.join(save_dir, '*epoch*.pth'))\n",
        "    if file_list:\n",
        "        epochs_exist = []\n",
        "        for file_ in file_list:\n",
        "            result = re.findall(\".*epoch(.*).pth.*\", file_)\n",
        "            epochs_exist.append(int(result[0]))\n",
        "        initial_epoch = max(epochs_exist)\n",
        "    else:\n",
        "        initial_epoch = 0\n",
        "    return initial_epoch\n",
        "\n",
        "\n",
        "def batch_PSNR(img, imclean, data_range):\n",
        "    Img = img.data.cpu().numpy().astype(np.float32)\n",
        "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
        "    PSNR = 0\n",
        "    for i in range(Img.shape[0]):\n",
        "        PSNR += compare_psnr(Iclean[i,:,:,:], Img[i,:,:,:], data_range=data_range)\n",
        "    return (PSNR/Img.shape[0])\n",
        "\n",
        "\n",
        "def normalize(data):\n",
        "    return data / 255.\n",
        "\n",
        "\n",
        "def is_image(img_name):\n",
        "    if img_name.endswith(\".jpg\") or img_name.endswith(\".bmp\") or img_name.endswith(\".png\"):\n",
        "        return True\n",
        "    else:\n",
        "        return  False\n",
        "\n",
        "\n",
        "def print_network(net):\n",
        "    num_params = 0\n",
        "    for param in net.parameters():\n",
        "        num_params += param.numel()\n",
        "    print(net)\n",
        "    print('Total number of parameters: %d' % num_params)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "szs80jb4Zn4V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch lib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "#Tools lib\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Dense_Layer(nn.Module):\n",
        "    def __init__(self, in_channels, growthrate, bn_size):\n",
        "        super(Dense_Layer, self).__init__()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, bn_size * growthrate, kernel_size=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(bn_size * growthrate)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            bn_size * growthrate, growthrate, kernel_size=3, padding=1, bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, prev_features):\n",
        "        out1 = torch.cat(prev_features, dim=1)\n",
        "        out1 = F.relu(out1)\n",
        "        out1 = self.conv1(out1)\n",
        "        out1 = F.relu(out1)\n",
        "        out2 = self.conv2(out1)\n",
        "        return out2\n",
        "\n",
        "class Dense_Block(nn.ModuleDict):\n",
        "    def __init__(self, n_layers, in_channels, growthrate, bn_size):\n",
        "        \"\"\"\n",
        "        A Dense block consists of `n_layers` of `Dense_Layer`\n",
        "        Parameters\n",
        "        ----------\n",
        "            n_layers: Number of dense layers to be stacked\n",
        "            in_channels: Number of input channels for first layer in the block\n",
        "            growthrate: Growth rate (k) as mentioned in DenseNet paper\n",
        "            bn_size: Multiplicative factor for # of bottleneck layers\n",
        "        \"\"\"\n",
        "        super(Dense_Block, self).__init__()\n",
        "\n",
        "        layers = dict()\n",
        "        for i in range(n_layers):\n",
        "            layer = Dense_Layer(in_channels + i * growthrate, growthrate, bn_size)\n",
        "            layers['dense{}'.format(i)] = layer\n",
        "\n",
        "        self.block = nn.ModuleDict(layers)\n",
        "\n",
        "    def forward(self, features):\n",
        "        if(isinstance(features, torch.Tensor)):\n",
        "            features = [features]\n",
        "\n",
        "        for _, layer in self.block.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "\n",
        "        return torch.cat(features, dim=1)\n",
        "\n",
        "\n",
        "class IteDNet(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(IteDNet, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.dense_block = nn.Sequential(\n",
        "            Dense_Block(6, 32, growthrate=32, bn_size=4),\n",
        "            )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(224, 3, 3, 1, 1),\n",
        "            )\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        x = input\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.iteration):\n",
        "            x = torch.cat((input, x), 1)\n",
        "            x = self.conv0(x)\n",
        "\n",
        "            x = self.dense_block(x)\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "class IReDNet(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(IReDNet, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.conv_i = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.conv_f = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.conv_g = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "            )\n",
        "        self.conv_o = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.dense_block = nn.Sequential(\n",
        "            Dense_Block(6, 32, growthrate=32, bn_size=4),\n",
        "            )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(224, 3, 3, 1, 1),\n",
        "            )\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, row, col = input.size(0), input.size(2), input.size(3)\n",
        "\n",
        "        x = input\n",
        "        h = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "        c = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "\n",
        "        if self.use_GPU:\n",
        "            h = h.cuda()\n",
        "            c = c.cuda()\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.iteration):\n",
        "            x = torch.cat((input, x), 1)\n",
        "            x = self.conv0(x)\n",
        "            x = torch.cat((x, h), 1)\n",
        "            i = self.conv_i(x)\n",
        "            f = self.conv_f(x)\n",
        "            g = self.conv_g(x)\n",
        "            o = self.conv_o(x)\n",
        "            c = f * c + i * g\n",
        "            h = o * torch.tanh(c)\n",
        "            x = h\n",
        "\n",
        "            x = self.dense_block(x)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "\n",
        "class IReDNet_LSTM(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(IReDNet_LSTM, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.conv_i = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.conv_f = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.conv_g = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "            )\n",
        "        self.conv_o = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.dense_block = nn.Sequential(\n",
        "            Dense_Block(6, 32, growthrate=32, bn_size=4),\n",
        "            )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(224, 3, 3, 1, 1),\n",
        "            )\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, row, col = input.size(0), input.size(2), input.size(3)\n",
        "\n",
        "        x = input\n",
        "        h = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "        c = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "\n",
        "        if self.use_GPU:\n",
        "            h = h.cuda()\n",
        "            c = c.cuda()\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.iteration):\n",
        "            x = torch.cat((input, x), 1)\n",
        "            x = self.conv0(x)\n",
        "            x = torch.cat((x, h), 1)\n",
        "            i = self.conv_i(x)\n",
        "            f = self.conv_f(x)\n",
        "            g = self.conv_g(x)\n",
        "            o = self.conv_o(x)\n",
        "            c = f * c + i * g\n",
        "            h = o * torch.tanh(c)\n",
        "            x = h\n",
        "\n",
        "            x = self.dense_block(x)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "class IReDNet_GRU(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(IReDNet_GRU, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.conv_z = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.conv_b = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.conv_g = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.dense_block = nn.Sequential(\n",
        "            Dense_Block(6, 32, growthrate=32, bn_size=4),\n",
        "            )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(224, 3, 3, 1, 1),\n",
        "            )\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, row, col = input.size(0), input.size(2), input.size(3)\n",
        "\n",
        "        x = input\n",
        "        h = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "        c = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "\n",
        "        if self.use_GPU:\n",
        "            h = h.cuda()\n",
        "            c = c.cuda()\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.iteration):\n",
        "            x = torch.cat((input, x), 1)\n",
        "            x = self.conv0(x)\n",
        "            x1 = torch.cat((x, h), 1)\n",
        "            z = self.conv_z(x1)\n",
        "            b = self.conv_b(x1)\n",
        "            s = b * h\n",
        "            s = torch.cat((s, x), 1)\n",
        "            g = self.conv_g(s)\n",
        "            h = (1 - z) * h + z * g\n",
        "\n",
        "            x = h\n",
        "\n",
        "            x = self.dense_block(x)\n",
        "            x = self.conv(x)\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "class IReDNet_BiRNN(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(IReDNet_BiRNN, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.conv_forward = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.conv_backward = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.dense_block = nn.Sequential(\n",
        "            Dense_Block(6, 32, growthrate=32, bn_size=4),\n",
        "            )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(224, 3, 3, 1, 1),\n",
        "            )\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, row, col = input.size(0), input.size(2), input.size(3)\n",
        "\n",
        "        x = input\n",
        "        h_forward = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "        h_backward = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "\n",
        "        if self.use_GPU:\n",
        "            h_forward = h_forward.cuda()\n",
        "            h_backward = h_backward.cuda()\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.iteration):\n",
        "            x = torch.cat((input, x), 1)\n",
        "            x = self.conv0(x)\n",
        "\n",
        "            # Forward pass\n",
        "            x_forward = torch.cat((x, h_forward), 1)\n",
        "            h_forward = self.conv_forward(x_forward)\n",
        "\n",
        "            # Backward pass\n",
        "            x_backward = torch.cat((x, h_backward), 1)\n",
        "            h_backward = self.conv_backward(x_backward)\n",
        "\n",
        "            # Combining forward and backward passes\n",
        "            x = h_forward + h_backward\n",
        "\n",
        "            x = self.dense_block(x)\n",
        "            x = self.conv(x)\n",
        "            x_list.append(x + input)  # Adding skip connection\n",
        "\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "class IReDNet_ConvLSTM(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(IReDNet_ConvLSTM, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # ConvLSTM Layers\n",
        "        self.convLSTM_i = nn.Conv2d(32 + 32, 32, 3, 1, 1)\n",
        "        self.convLSTM_f = nn.Conv2d(32 + 32, 32, 3, 1, 1)\n",
        "        self.convLSTM_c = nn.Conv2d(32 + 32, 32, 3, 1, 1)\n",
        "        self.convLSTM_o = nn.Conv2d(32 + 32, 32, 3, 1, 1)\n",
        "\n",
        "        # Dense Block\n",
        "        self.dense_block = Dense_Block(6, 32, growthrate=32, bn_size=4)\n",
        "\n",
        "        # Output convolutional layer\n",
        "        self.conv = nn.Conv2d(224, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, row, col = input.size(0), input.size(2), input.size(3)\n",
        "\n",
        "        x = input\n",
        "        h = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "        c = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "\n",
        "        if self.use_GPU:\n",
        "            h = h.cuda()\n",
        "            c = c.cuda()\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.iteration):\n",
        "            x = torch.cat((input, x), 1)\n",
        "            x = self.conv0(x)\n",
        "            combined = torch.cat((x, h), 1)\n",
        "\n",
        "            # ConvLSTM calculations\n",
        "            i = torch.sigmoid(self.convLSTM_i(combined))\n",
        "            f = torch.sigmoid(self.convLSTM_f(combined))\n",
        "            o = torch.sigmoid(self.convLSTM_o(combined))\n",
        "            c_tilde = torch.tanh(self.convLSTM_c(combined))\n",
        "            c = f * c + i * c_tilde\n",
        "            h = o * torch.tanh(c)\n",
        "\n",
        "            x = h\n",
        "            x = self.dense_block(x)\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "\n",
        "class IReDNet_IndRNN(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(IReDNet_IndRNN, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # IndRNN layers\n",
        "        self.recurrent_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(32 + 32, 32, 3, 1, 1),  # Convolution for each IndRNN cell\n",
        "                nn.ReLU()\n",
        "            ) for _ in range(self.iteration)\n",
        "        ])\n",
        "\n",
        "        # Dense Block\n",
        "        self.dense_block = nn.Sequential(\n",
        "            Dense_Block(6, 32, growthrate=32, bn_size=4),\n",
        "        )\n",
        "\n",
        "        # Final convolutional layer\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(224, 3, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, row, col = input.size(0), input.size(2), input.size(3)\n",
        "\n",
        "        x = input\n",
        "        h = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "        if self.use_GPU:\n",
        "            h = h.cuda()\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.iteration):\n",
        "            combined = torch.cat((input, x), 1)\n",
        "            combined = self.conv0(combined)\n",
        "            combined = torch.cat((combined, h), 1)\n",
        "            h = self.recurrent_layers[i](combined)\n",
        "\n",
        "            x = h\n",
        "\n",
        "            x = self.dense_block(x)\n",
        "            x = self.conv(x)\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "\n",
        "class IReDNet_QRNN(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(IReDNet_QRNN, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # QRNN layers\n",
        "        self.conv_f = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.conv_z = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.conv_o = nn.Sequential(\n",
        "            nn.Conv2d(32 + 32, 32, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Dense Block\n",
        "        self.dense_block = nn.Sequential(\n",
        "            Dense_Block(6, 32, growthrate=32, bn_size=4),\n",
        "        )\n",
        "\n",
        "        # Final Convolution\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(224, 3, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, row, col = input.size(0), input.size(2), input.size(3)\n",
        "\n",
        "        x = input\n",
        "        h = Variable(torch.zeros(batch_size, 32, row, col))\n",
        "\n",
        "        if self.use_GPU:\n",
        "            h = h.cuda()\n",
        "\n",
        "        x_list = []\n",
        "        for _ in range(self.iteration):\n",
        "            x = torch.cat((input, x), 1)\n",
        "            x = self.conv0(x)\n",
        "\n",
        "            combined = torch.cat((x, h), 1)\n",
        "            f = self.conv_f(combined)\n",
        "            z = self.conv_z(combined)\n",
        "            o = self.conv_o(combined)\n",
        "\n",
        "            h = (1 - z) * h + z * o\n",
        "\n",
        "            x = self.dense_block(h)\n",
        "            x = self.conv(x)\n",
        "\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "\n",
        "        return x, x_list\n",
        "\n"
      ],
      "metadata": {
        "id": "BpEK0ZZlikog"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch lib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "#Tools lib\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Light_Dense_Layer(nn.Module):\n",
        "    def __init__(self, in_channels, growthrate):\n",
        "        super(Light_Dense_Layer, self).__init__()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels, growthrate // 2, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(growthrate // 2)\n",
        "        self.conv2 = nn.Conv2d(growthrate // 2, growthrate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, prev_features):\n",
        "        out1 = torch.cat(prev_features, dim=1)\n",
        "        out1 = F.relu(self.bn1(out1))\n",
        "        out1 = self.conv1(out1)\n",
        "        out1 = F.relu(self.bn2(out1))\n",
        "        out2 = self.conv2(out1)\n",
        "        return out2\n",
        "\n",
        "class Light_Dense_Block(nn.Module):\n",
        "    def __init__(self, n_layers, in_channels, growthrate):\n",
        "        super(Light_Dense_Block, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(n_layers):\n",
        "            layers.append(Light_Dense_Layer(in_channels + i * growthrate, growthrate))\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, features):\n",
        "        if isinstance(features, torch.Tensor):\n",
        "            features = [features]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "\n",
        "        return torch.cat(features, dim=1)\n",
        "\n",
        "class LightIteDNet(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(LightIteDNet, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.dense_block = Light_Dense_Block(4, 16, growthrate=16)\n",
        "        self.conv = nn.Conv2d(80, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.use_GPU:\n",
        "            input = input.cuda()\n",
        "\n",
        "        x = input\n",
        "\n",
        "        x_list = []\n",
        "        for _ in range(self.iteration):\n",
        "            combined = torch.cat((input, x), 1)\n",
        "            combined = self.conv0(combined)\n",
        "\n",
        "            x = self.dense_block(combined)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "class LightIReDNet(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(LightIReDNet, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.conv_i = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.conv_f = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.conv_g = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "            )\n",
        "        self.conv_o = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.dense_block = Light_Dense_Block(4, 16, growthrate=16)\n",
        "        self.conv = nn.Conv2d(80, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.use_GPU:\n",
        "            input = input.cuda()\n",
        "\n",
        "        x = input\n",
        "        h = torch.zeros(input.size(0), 16, input.size(2), input.size(3), device=input.device)\n",
        "        c = torch.zeros_like(h)\n",
        "\n",
        "        x_list = []\n",
        "        for _ in range(self.iteration):\n",
        "            combined = torch.cat((input, x), 1)\n",
        "            combined = self.conv0(combined)\n",
        "            combined = torch.cat((combined, h), 1)\n",
        "            i = self.conv_i(combined)\n",
        "            f = self.conv_f(combined)\n",
        "            g = self.conv_g(combined)\n",
        "            o = self.conv_o(combined)\n",
        "            c = f * c + i * g\n",
        "            h = o * torch.tanh(c)\n",
        "\n",
        "            x = self.dense_block(h)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "\n",
        "class LightIReDNet_LSTM(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(LightIReDNet_LSTM, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.conv_i = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.conv_f = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.conv_g = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "            )\n",
        "        self.conv_o = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        self.dense_block = Light_Dense_Block(4, 16, growthrate=16)\n",
        "        self.conv = nn.Conv2d(80, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.use_GPU:\n",
        "            input = input.cuda()\n",
        "\n",
        "        x = input\n",
        "        h = torch.zeros(input.size(0), 16, input.size(2), input.size(3), device=input.device)\n",
        "        c = torch.zeros_like(h)\n",
        "\n",
        "        x_list = []\n",
        "        for _ in range(self.iteration):\n",
        "            combined = torch.cat((input, x), 1)\n",
        "            combined = self.conv0(combined)\n",
        "            combined = torch.cat((combined, h), 1)\n",
        "            i = self.conv_i(combined)\n",
        "            f = self.conv_f(combined)\n",
        "            g = self.conv_g(combined)\n",
        "            o = self.conv_o(combined)\n",
        "            c = f * c + i * g\n",
        "            h = o * torch.tanh(c)\n",
        "            x = h\n",
        "            x = self.dense_block(x)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "class LightIReDNet_GRU(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(LightIReDNet_GRU, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        # Initial Convolutional Layer\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # GRU-like gates using Convolution\n",
        "        self.conv_z = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.conv_r = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.conv_h = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Dense Block\n",
        "        self.dense_block = Light_Dense_Block(4, 16, growthrate=16)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.conv = nn.Conv2d(80, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.use_GPU:\n",
        "            input = input.cuda()\n",
        "\n",
        "        x = input\n",
        "        h = torch.zeros(input.size(0), 16, input.size(2), input.size(3), device=input.device)\n",
        "\n",
        "        x_list = []\n",
        "        for _ in range(self.iteration):\n",
        "            combined = torch.cat((input, x), 1)\n",
        "            combined = self.conv0(combined)\n",
        "            combined_with_h = torch.cat((combined, h), 1)\n",
        "\n",
        "            z = self.conv_z(combined_with_h)\n",
        "            r = self.conv_r(combined_with_h)\n",
        "\n",
        "            combined_reset = torch.cat((combined, r * h), 1)\n",
        "            h_tilde = self.conv_h(combined_reset)\n",
        "\n",
        "            h = (1 - z) * h + z * h_tilde\n",
        "\n",
        "            x = self.dense_block(h)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "class LightIReDNet_BiRNN(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(LightIReDNet_BiRNN, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        # Initial Convolutional Layer\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Forward and backward layers\n",
        "        self.conv_forward = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.conv_backward = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Dense Block\n",
        "        self.dense_block = Light_Dense_Block(4, 16, growthrate=16)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.conv = nn.Conv2d(80, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.use_GPU:\n",
        "            input = input.cuda()\n",
        "\n",
        "        x = input\n",
        "        h_forward = torch.zeros(input.size(0), 16, input.size(2), input.size(3), device=input.device)\n",
        "        h_backward = torch.zeros_like(h_forward)\n",
        "\n",
        "        x_list = []\n",
        "        for _ in range(self.iteration):\n",
        "            combined = torch.cat((input, x), 1)\n",
        "            combined = self.conv0(combined)\n",
        "\n",
        "            # Forward pass\n",
        "            combined_forward = torch.cat((combined, h_forward), 1)\n",
        "            h_forward = self.conv_forward(combined_forward)\n",
        "\n",
        "            # Backward pass\n",
        "            combined_backward = torch.cat((combined, h_backward), 1)\n",
        "            h_backward = self.conv_backward(combined_backward)\n",
        "\n",
        "            # Combining forward and backward passes\n",
        "            h_combined = h_forward + h_backward\n",
        "\n",
        "            x = self.dense_block(h_combined)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "\n",
        "class LightIReDNet_IndRNN(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(LightIReDNet_IndRNN, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        # Initial Convolutional Layer\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Independently Recurrent Convolutional Layer\n",
        "        self.indrnn = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Dense Block\n",
        "        self.dense_block = Light_Dense_Block(4, 16, growthrate=16)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.conv = nn.Conv2d(80, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.use_GPU:\n",
        "            input = input.cuda()\n",
        "\n",
        "        x = input\n",
        "        h = torch.zeros(input.size(0), 16, input.size(2), input.size(3), device=input.device)\n",
        "\n",
        "        x_list = []\n",
        "        for _ in range(self.iteration):\n",
        "            combined = torch.cat((input, x), 1)\n",
        "            combined = self.conv0(combined)\n",
        "            combined = torch.cat((combined, h), 1)\n",
        "\n",
        "            h = self.indrnn(combined)\n",
        "\n",
        "            x = self.dense_block(h)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n",
        "\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "        \"\"\"\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias = bias\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def forward(self, x, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "        combined = torch.cat([x, h_cur], dim=1)\n",
        "        combined_conv = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, image_size):\n",
        "        height, width = image_size\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
        "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
        "\n",
        "\n",
        "class LightIReDNet_ConvLSTM(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(LightIReDNet_ConvLSTM, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.convLSTM = ConvLSTMCell(input_dim=16, hidden_dim=16, kernel_size=(3, 3), bias=True)\n",
        "\n",
        "        self.dense_block = Light_Dense_Block(4, 16, growthrate=16)\n",
        "\n",
        "        self.conv = nn.Conv2d(80, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.use_GPU:\n",
        "            input = input.cuda()\n",
        "\n",
        "        x = input\n",
        "        h, c = self.convLSTM.init_hidden(input.size(0), (input.size(2), input.size(3)))\n",
        "\n",
        "        x_list = []\n",
        "        for _ in range(self.iteration):\n",
        "            combined = torch.cat((input, x), 1)\n",
        "            combined = self.conv0(combined)\n",
        "\n",
        "            h, c = self.convLSTM(combined, (h, c))\n",
        "            x = self.dense_block(h)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "\n",
        "        return x, x_list\n",
        "\n",
        "class LightIReDNet_QRNN(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(LightIReDNet_QRNN, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "        self.use_GPU = use_GPU\n",
        "\n",
        "        # Initial Convolutional Layer\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # QRNN-like gates using Convolution\n",
        "        self.conv_f = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.conv_z = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.conv_o = nn.Sequential(\n",
        "            nn.Conv2d(16 + 16, 16, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Dense Block\n",
        "        self.dense_block = Light_Dense_Block(4, 16, growthrate=16)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.conv = nn.Conv2d(80, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.use_GPU:\n",
        "            input = input.cuda()\n",
        "\n",
        "        x = input\n",
        "        h = torch.zeros(input.size(0), 16, input.size(2), input.size(3), device=input.device)\n",
        "\n",
        "        x_list = []\n",
        "        for _ in range(self.iteration):\n",
        "            combined = torch.cat((input, x), 1)\n",
        "            combined = self.conv0(combined)\n",
        "            combined_with_h = torch.cat((combined, h), 1)\n",
        "\n",
        "            f = self.conv_f(combined_with_h)\n",
        "            z = self.conv_z(combined_with_h)\n",
        "            o = self.conv_o(combined_with_h)\n",
        "\n",
        "            h = (f * h) + ((1 - f) * z)\n",
        "\n",
        "            x = self.dense_block(o * h)\n",
        "\n",
        "            x = self.conv(x)\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "        return x, x_list\n"
      ],
      "metadata": {
        "id": "VrX1JQl4ivCo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import random\n",
        "import h5py\n",
        "import torch\n",
        "import cv2\n",
        "import glob\n",
        "import torch.utils.data as udata\n",
        "\n",
        "def Im2Patch(img, win, stride=1):\n",
        "    k = 0\n",
        "    endc = img.shape[0]\n",
        "    endw = img.shape[1]\n",
        "    endh = img.shape[2]\n",
        "    patch = img[:, 0:endw - win + 0 + 1:stride, 0:endh - win + 0 + 1:stride]\n",
        "    TotalPatNum = patch.shape[1] * patch.shape[2]\n",
        "    Y = np.zeros([endc, win * win, TotalPatNum], np.float32)\n",
        "\n",
        "    for i in range(win):\n",
        "        for j in range(win):\n",
        "            patch = img[:, i:endw - win + i + 1:stride, j:endh - win + j + 1:stride]\n",
        "            Y[:, k, :] = np.array(patch[:]).reshape(endc, TotalPatNum)\n",
        "            k = k + 1\n",
        "    return Y.reshape([endc, win, win, TotalPatNum])\n",
        "\n",
        "\n",
        "def prepare_data_Rain12600(data_path, patch_size, stride):\n",
        "    # train\n",
        "    print('process training data')\n",
        "    input_path = os.path.join(data_path, 'rainy_image')\n",
        "    target_path = os.path.join(data_path, 'ground_truth')\n",
        "\n",
        "    save_target_path = os.path.join(data_path, 'train_target.h5')\n",
        "    save_input_path = os.path.join(data_path, 'train_input.h5')\n",
        "\n",
        "    target_h5f = h5py.File(save_target_path, 'w')\n",
        "    input_h5f = h5py.File(save_input_path, 'w')\n",
        "\n",
        "    train_num = 0\n",
        "    for i in range(900):\n",
        "        target_file = \"%d.jpg\" % (i + 1)\n",
        "        target = cv2.imread(os.path.join(target_path,target_file))\n",
        "        b, g, r = cv2.split(target)\n",
        "        target = cv2.merge([r, g, b])\n",
        "\n",
        "        for j in range(14):\n",
        "            input_file = \"%d_%d.jpg\" % (i+1, j+1)\n",
        "            input_img = cv2.imread(os.path.join(input_path,input_file))\n",
        "            b, g, r = cv2.split(input_img)\n",
        "            input_img = cv2.merge([r, g, b])\n",
        "\n",
        "            target_img = target\n",
        "            target_img = np.float32(normalize(target_img))\n",
        "            target_patches = Im2Patch(target_img.transpose(2,0,1), win=patch_size, stride=stride)\n",
        "\n",
        "            input_img = np.float32(normalize(input_img))\n",
        "            input_patches = Im2Patch(input_img.transpose(2, 0, 1), win=patch_size, stride=stride)\n",
        "            print(\"target file: %s # samples: %d\" % (input_file, target_patches.shape[3]))\n",
        "\n",
        "            for n in range(target_patches.shape[3]):\n",
        "                target_data = target_patches[:, :, :, n].copy()\n",
        "                target_h5f.create_dataset(str(train_num), data=target_data)\n",
        "\n",
        "                input_data = input_patches[:, :, :, n].copy()\n",
        "                input_h5f.create_dataset(str(train_num), data=input_data)\n",
        "                train_num += 1\n",
        "\n",
        "    target_h5f.close()\n",
        "    input_h5f.close()\n",
        "    print('training set, # samples %d\\n' % train_num)\n",
        "\n",
        "\n",
        "def prepare_data_RainTrainH(data_path, patch_size, stride):\n",
        "    # train\n",
        "    print('process training data')\n",
        "    input_path = os.path.join(data_path)\n",
        "    target_path = os.path.join(data_path)\n",
        "\n",
        "    save_target_path = os.path.join(data_path, 'train_target.h5')\n",
        "    save_input_path = os.path.join(data_path, 'train_input.h5')\n",
        "\n",
        "    target_h5f = h5py.File(save_target_path, 'w')\n",
        "    input_h5f = h5py.File(save_input_path, 'w')\n",
        "\n",
        "    train_num = 0\n",
        "    for i in range(1800):\n",
        "        target_file = \"norain-%d.png\" % (i + 1)\n",
        "        if os.path.exists(os.path.join(target_path,target_file)):\n",
        "\n",
        "            target = cv2.imread(os.path.join(target_path,target_file))\n",
        "            b, g, r = cv2.split(target)\n",
        "            target = cv2.merge([r, g, b])\n",
        "\n",
        "            input_file = \"rain-%d.png\" % (i + 1)\n",
        "\n",
        "            if os.path.exists(os.path.join(input_path,input_file)): # we delete 546 samples\n",
        "\n",
        "                input_img = cv2.imread(os.path.join(input_path,input_file))\n",
        "                b, g, r = cv2.split(input_img)\n",
        "                input_img = cv2.merge([r, g, b])\n",
        "\n",
        "                target_img = target\n",
        "                target_img = np.float32(normalize(target_img))\n",
        "                target_patches = Im2Patch(target_img.transpose(2,0,1), win=patch_size, stride=stride)\n",
        "\n",
        "                input_img = np.float32(normalize(input_img))\n",
        "                input_patches = Im2Patch(input_img.transpose(2, 0, 1), win=patch_size, stride=stride)\n",
        "\n",
        "                print(\"target file: %s # samples: %d\" % (input_file, target_patches.shape[3]))\n",
        "\n",
        "                for n in range(target_patches.shape[3]):\n",
        "                    target_data = target_patches[:, :, :, n].copy()\n",
        "                    target_h5f.create_dataset(str(train_num), data=target_data)\n",
        "\n",
        "                    input_data = input_patches[:, :, :, n].copy()\n",
        "                    input_h5f.create_dataset(str(train_num), data=input_data)\n",
        "\n",
        "                    train_num += 1\n",
        "\n",
        "    target_h5f.close()\n",
        "    input_h5f.close()\n",
        "\n",
        "    print('training set, # samples %d\\n' % train_num)\n",
        "\n",
        "\n",
        "def prepare_data_RainTrainL(data_path, patch_size, stride):\n",
        "    # train\n",
        "    print('process training data')\n",
        "    input_path = os.path.join(data_path)\n",
        "    target_path = os.path.join(data_path)\n",
        "\n",
        "    save_target_path = os.path.join(data_path, 'train_target.h5')\n",
        "    save_input_path = os.path.join(data_path, 'train_input.h5')\n",
        "\n",
        "    target_h5f = h5py.File(save_target_path, 'w')\n",
        "    input_h5f = h5py.File(save_input_path, 'w')\n",
        "\n",
        "    train_num = 0\n",
        "    for i in range(200):\n",
        "        target_file = \"norain-%d.png\" % (i + 1)\n",
        "        target = cv2.imread(os.path.join(target_path,target_file))\n",
        "        b, g, r = cv2.split(target)\n",
        "        target = cv2.merge([r, g, b])\n",
        "\n",
        "        for j in range(2):\n",
        "            input_file = \"rain-%d.png\" % (i + 1)\n",
        "            input_img = cv2.imread(os.path.join(input_path,input_file))\n",
        "            b, g, r = cv2.split(input_img)\n",
        "            input_img = cv2.merge([r, g, b])\n",
        "\n",
        "            target_img = target\n",
        "\n",
        "            if j == 1:\n",
        "                target_img = cv2.flip(target_img, 1)\n",
        "                input_img = cv2.flip(input_img, 1)\n",
        "\n",
        "            target_img = np.float32(normalize(target_img))\n",
        "            target_patches = Im2Patch(target_img.transpose(2,0,1), win=patch_size, stride=stride)\n",
        "\n",
        "            input_img = np.float32(normalize(input_img))\n",
        "            input_patches = Im2Patch(input_img.transpose(2, 0, 1), win=patch_size, stride=stride)\n",
        "\n",
        "            print(\"target file: %s # samples: %d\" % (input_file, target_patches.shape[3]))\n",
        "            for n in range(target_patches.shape[3]):\n",
        "                target_data = target_patches[:, :, :, n].copy()\n",
        "                target_h5f.create_dataset(str(train_num), data=target_data)\n",
        "\n",
        "                input_data = input_patches[:, :, :, n].copy()\n",
        "                input_h5f.create_dataset(str(train_num), data=input_data)\n",
        "\n",
        "                train_num += 1\n",
        "\n",
        "    target_h5f.close()\n",
        "    input_h5f.close()\n",
        "\n",
        "    print('training set, # samples %d\\n' % train_num)\n",
        "\n",
        "\n",
        "class Dataset(udata.Dataset):\n",
        "    def __init__(self, data_path='.'):\n",
        "        super(Dataset, self).__init__()\n",
        "\n",
        "        self.data_path = data_path\n",
        "\n",
        "        target_path = os.path.join(self.data_path, 'train_target.h5')\n",
        "        input_path = os.path.join(self.data_path, 'train_input.h5')\n",
        "\n",
        "        target_h5f = h5py.File(target_path, 'r')\n",
        "        input_h5f = h5py.File(input_path, 'r')\n",
        "\n",
        "        self.keys = list(target_h5f.keys())\n",
        "        random.shuffle(self.keys)\n",
        "        target_h5f.close()\n",
        "        input_h5f.close()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        target_path = os.path.join(self.data_path, 'train_target.h5')\n",
        "        input_path = os.path.join(self.data_path, 'train_input.h5')\n",
        "\n",
        "        target_h5f = h5py.File(target_path, 'r')\n",
        "        input_h5f = h5py.File(input_path, 'r')\n",
        "\n",
        "        key = self.keys[index]\n",
        "        target = np.array(target_h5f[key])\n",
        "        input = np.array(input_h5f[key])\n",
        "\n",
        "        target_h5f.close()\n",
        "        input_h5f.close()\n",
        "\n",
        "        return torch.Tensor(input), torch.Tensor(target)"
      ],
      "metadata": {
        "id": "1LM8FViZ7AP6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from math import exp\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
        "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
        "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size = 11, size_average = True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel)\n",
        "\n",
        "            if img1.is_cuda:\n",
        "                window = window.cuda(img1.get_device())\n",
        "            window = window.type_as(img1)\n",
        "\n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
        "\n",
        "def ssim(img1, img2, window_size = 11, size_average = True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "\n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "\n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)"
      ],
      "metadata": {
        "id": "nc_N7bKU7ab9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    r\"\"\"Implements Adam algorithm.\n",
        "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, amsgrad=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        super(Adam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "                else:\n",
        "                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                step_size = group['lr'] / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "ypgWR-mM7gLA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMNQCKtrjjU-",
        "outputId": "0d2f6334-99c3-4a36-cd7d-539bc6574931"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset"
      ],
      "metadata": {
        "id": "C_qP1ttszBOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1ktuaInh9Lsnxxdml8f3x4NASeZe6I0Gc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8X9v96RjmFJ",
        "outputId": "e8bd9ee0-0a42-4fbb-e340-3fc33e34e384"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ktuaInh9Lsnxxdml8f3x4NASeZe6I0Gc\n",
            "To: /content/datasets_Synthetic_PReNet.zip\n",
            "100% 2.16G/2.16G [00:33<00:00, 63.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/datasets_Synthetic_PReNet.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sboRYTmqmUvo",
        "outputId": "7e965f28-0250-4997-f32e-644d09ee80a7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/datasets_Synthetic_PReNet.zip\n",
            "   creating: /content/datasets/\n",
            "  inflating: /content/datasets/train.zip  \n",
            "  inflating: /content/datasets/test.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/datasets/test.zip -d /content/datasets/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xox4W3XXoem-",
        "outputId": "2cde97cc-c5f8-4791-a164-1876fbada804"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/datasets/test.zip\n",
            "   creating: /content/datasets/test/\n",
            "  inflating: /content/datasets/test/Rain100H.zip  \n",
            "  inflating: /content/datasets/test/Rain100L.zip  \n",
            "  inflating: /content/datasets/test/Rain1400.zip  \n",
            "  inflating: /content/datasets/test/real.zip  \n",
            "  inflating: /content/datasets/test/test12.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/datasets/test/Rain100L.zip -d /content/datasets/test/\n",
        "!unzip /content/datasets/test/Rain100H.zip -d /content/datasets/test/\n",
        "!unzip /content/datasets/test/Rain1400.zip -d /content/datasets/test/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDdVFmfSwU-X",
        "outputId": "b0ac3469-44c3-46b1-f8b5-4252d08a057b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/datasets/test/Rain100L.zip\n",
            "   creating: /content/datasets/test/Rain100L/\n",
            "  inflating: /content/datasets/test/Rain100L/norain-001.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-002.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-003.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-004.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-005.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-006.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-007.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-008.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-009.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-010.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-011.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-012.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-013.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-014.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-015.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-016.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-017.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-018.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-019.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-020.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-021.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-022.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-023.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-024.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-025.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-026.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-027.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-028.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-029.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-030.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-031.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-032.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-033.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-034.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-035.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-036.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-037.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-038.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-039.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-040.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-041.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-042.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-043.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-044.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-045.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-046.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-047.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-048.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-049.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-050.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-051.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-052.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-053.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-054.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-055.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-056.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-057.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-058.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-059.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-060.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-061.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-062.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-063.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-064.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-065.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-066.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-067.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-068.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-069.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-070.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-071.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-072.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-073.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-074.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-075.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-076.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-077.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-078.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-079.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-080.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-081.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-082.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-083.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-084.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-085.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-086.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-087.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-088.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-089.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-090.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-091.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-092.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-093.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-094.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-095.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-096.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-097.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-098.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-099.png  \n",
            "  inflating: /content/datasets/test/Rain100L/norain-100.png  \n",
            "   creating: /content/datasets/test/Rain100L/rainy/\n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-001.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-002.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-003.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-004.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-005.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-006.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-007.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-008.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-009.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-010.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-011.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-012.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-013.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-014.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-015.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-016.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-017.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-018.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-019.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-020.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-021.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-022.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-023.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-024.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-025.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-026.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-027.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-028.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-029.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-030.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-031.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-032.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-033.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-034.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-035.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-036.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-037.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-038.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-039.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-040.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-041.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-042.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-043.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-044.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-045.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-046.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-047.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-048.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-049.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-050.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-051.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-052.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-053.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-054.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-055.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-056.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-057.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-058.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-059.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-060.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-061.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-062.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-063.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-064.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-065.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-066.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-067.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-068.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-069.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-070.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-071.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-072.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-073.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-074.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-075.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-076.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-077.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-078.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-079.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-080.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-081.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-082.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-083.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-084.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-085.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-086.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-087.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-088.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-089.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-090.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-091.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-092.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-093.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-094.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-095.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-096.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-097.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-098.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-099.png  \n",
            "  inflating: /content/datasets/test/Rain100L/rainy/rain-100.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/datasets/train.zip -d /content/datasets/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqzTXEVEwRin",
        "outputId": "d760039b-852f-4735-a5d5-f861de9401c3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/datasets/train.zip\n",
            "   creating: /content/datasets/train/\n",
            "  inflating: /content/datasets/train/Rain12600.zip  \n",
            "  inflating: /content/datasets/train/RainTrainH.zip  \n",
            "  inflating: /content/datasets/train/RainTrainL.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/datasets/train/RainTrainL.zip -d /content/datasets/train/\n",
        "# !unzip /content/datasets/train/RainTrainH.zip -d /content/datasets/train/\n",
        "# !unzip /content/datasets/train/Rain12600.zip -d /content/datasets/train/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syk4RB43zOC8",
        "outputId": "d721d976-1d77-4a00-98a2-7464f3f1005e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/datasets/train/RainTrainL.zip\n",
            "   creating: /content/datasets/train/RainTrainL/\n",
            "  inflating: /content/datasets/train/RainTrainL/norain-1.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-10.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-100.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-101.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-102.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-103.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-104.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-105.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-106.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-107.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-108.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-109.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-11.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-110.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-111.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-112.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-113.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-114.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-115.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-116.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-117.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-118.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-119.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-12.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-120.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-121.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-122.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-123.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-124.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-125.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-126.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-127.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-128.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-129.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-13.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-130.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-131.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-132.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-133.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-134.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-135.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-136.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-137.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-138.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-139.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-14.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-140.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-141.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-142.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-143.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-144.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-145.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-146.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-147.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-148.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-149.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-15.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-150.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-151.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-152.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-153.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-154.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-155.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-156.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-157.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-158.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-159.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-16.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-160.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-161.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-162.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-163.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-164.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-165.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-166.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-167.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-168.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-169.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-17.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-170.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-171.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-172.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-173.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-174.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-175.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-176.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-177.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-178.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-179.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-18.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-180.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-181.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-182.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-183.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-184.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-185.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-186.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-187.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-188.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-189.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-19.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-190.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-191.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-192.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-193.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-194.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-195.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-196.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-197.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-198.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-199.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-2.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-20.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-200.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-21.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-22.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-23.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-24.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-25.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-26.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-27.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-28.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-29.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-3.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-30.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-31.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-32.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-33.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-34.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-35.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-36.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-37.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-38.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-39.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-4.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-40.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-41.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-42.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-43.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-44.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-45.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-46.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-47.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-48.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-49.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-5.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-50.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-51.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-52.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-53.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-54.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-55.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-56.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-57.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-58.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-59.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-6.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-60.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-61.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-62.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-63.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-64.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-65.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-66.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-67.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-68.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-69.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-7.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-70.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-71.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-72.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-73.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-74.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-75.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-76.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-77.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-78.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-79.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-8.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-80.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-81.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-82.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-83.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-84.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-85.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-86.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-87.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-88.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-89.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-9.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-90.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-91.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-92.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-93.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-94.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-95.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-96.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-97.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-98.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/norain-99.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-1.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-10.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-100.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-101.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-102.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-103.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-104.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-105.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-106.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-107.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-108.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-109.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-11.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-110.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-111.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-112.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-113.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-114.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-115.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-116.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-117.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-118.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-119.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-12.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-120.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-121.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-122.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-123.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-124.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-125.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-126.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-127.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-128.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-129.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-13.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-130.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-131.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-132.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-133.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-134.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-135.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-136.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-137.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-138.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-139.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-14.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-140.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-141.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-142.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-143.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-144.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-145.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-146.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-147.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-148.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-149.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-15.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-150.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-151.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-152.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-153.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-154.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-155.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-156.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-157.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-158.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-159.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-16.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-160.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-161.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-162.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-163.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-164.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-165.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-166.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-167.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-168.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-169.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-17.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-170.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-171.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-172.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-173.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-174.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-175.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-176.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-177.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-178.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-179.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-18.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-180.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-181.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-182.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-183.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-184.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-185.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-186.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-187.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-188.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-189.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-19.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-190.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-191.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-192.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-193.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-194.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-195.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-196.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-197.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-198.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-199.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-2.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-20.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-200.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-21.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-22.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-23.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-24.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-25.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-26.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-27.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-28.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-29.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-3.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-30.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-31.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-32.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-33.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-34.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-35.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-36.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-37.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-38.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-39.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-4.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-40.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-41.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-42.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-43.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-44.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-45.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-46.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-47.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-48.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-49.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-5.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-50.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-51.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-52.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-53.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-54.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-55.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-56.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-57.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-58.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-59.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-6.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-60.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-61.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-62.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-63.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-64.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-65.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-66.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-67.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-68.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-69.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-7.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-70.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-71.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-72.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-73.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-74.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-75.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-76.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-77.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-78.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-79.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-8.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-80.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-81.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-82.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-83.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-84.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-85.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-86.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-87.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-88.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-89.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-9.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-90.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-91.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-92.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-93.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-94.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-95.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-96.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-97.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-98.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rain-99.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-1.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-10.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-100.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-101.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-102.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-103.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-104.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-105.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-106.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-107.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-108.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-109.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-11.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-110.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-111.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-112.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-113.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-114.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-115.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-116.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-117.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-118.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-119.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-12.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-120.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-121.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-122.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-123.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-124.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-125.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-126.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-127.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-128.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-129.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-13.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-130.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-131.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-132.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-133.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-134.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-135.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-136.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-137.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-138.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-139.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-14.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-140.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-141.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-142.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-143.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-144.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-145.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-146.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-147.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-148.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-149.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-15.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-150.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-151.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-152.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-153.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-154.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-155.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-156.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-157.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-158.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-159.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-16.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-160.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-161.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-162.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-163.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-164.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-165.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-166.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-167.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-168.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-169.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-17.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-170.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-171.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-172.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-173.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-174.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-175.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-176.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-177.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-178.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-179.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-18.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-180.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-181.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-182.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-183.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-184.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-185.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-186.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-187.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-188.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-189.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-19.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-190.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-191.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-192.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-193.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-194.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-195.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-196.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-197.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-198.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-199.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-2.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-20.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-200.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-21.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-22.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-23.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-24.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-25.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-26.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-27.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-28.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-29.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-3.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-30.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-31.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-32.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-33.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-34.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-35.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-36.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-37.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-38.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-39.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-4.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-40.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-41.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-42.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-43.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-44.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-45.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-46.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-47.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-48.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-49.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-5.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-50.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-51.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-52.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-53.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-54.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-55.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-56.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-57.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-58.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-59.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-6.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-60.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-61.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-62.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-63.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-64.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-65.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-66.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-67.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-68.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-69.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-7.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-70.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-71.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-72.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-73.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-74.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-75.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-76.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-77.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-78.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-79.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-8.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-80.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-81.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-82.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-83.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-84.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-85.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-86.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-87.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-88.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-89.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-9.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-90.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-91.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-92.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-93.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-94.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-95.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-96.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-97.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-98.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainregion-99.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-1.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-10.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-100.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-101.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-102.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-103.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-104.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-105.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-106.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-107.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-108.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-109.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-11.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-110.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-111.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-112.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-113.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-114.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-115.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-116.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-117.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-118.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-119.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-12.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-120.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-121.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-122.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-123.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-124.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-125.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-126.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-127.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-128.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-129.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-13.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-130.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-131.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-132.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-133.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-134.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-135.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-136.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-137.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-138.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-139.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-14.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-140.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-141.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-142.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-143.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-144.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-145.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-146.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-147.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-148.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-149.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-15.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-150.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-151.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-152.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-153.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-154.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-155.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-156.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-157.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-158.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-159.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-16.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-160.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-161.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-162.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-163.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-164.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-165.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-166.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-167.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-168.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-169.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-17.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-170.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-171.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-172.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-173.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-174.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-175.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-176.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-177.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-178.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-179.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-18.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-180.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-181.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-182.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-183.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-184.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-185.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-186.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-187.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-188.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-189.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-19.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-190.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-191.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-192.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-193.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-194.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-195.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-196.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-197.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-198.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-199.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-2.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-20.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-200.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-21.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-22.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-23.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-24.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-25.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-26.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-27.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-28.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-29.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-3.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-30.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-31.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-32.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-33.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-34.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-35.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-36.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-37.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-38.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-39.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-4.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-40.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-41.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-42.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-43.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-44.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-45.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-46.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-47.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-48.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-49.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-5.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-50.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-51.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-52.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-53.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-54.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-55.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-56.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-57.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-58.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-59.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-6.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-60.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-61.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-62.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-63.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-64.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-65.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-66.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-67.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-68.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-69.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-7.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-70.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-71.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-72.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-73.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-74.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-75.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-76.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-77.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-78.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-79.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-8.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-80.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-81.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-82.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-83.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-84.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-85.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-86.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-87.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-88.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-89.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-9.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-90.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-91.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-92.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-93.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-94.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-95.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-96.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-97.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-98.png  \n",
            "  inflating: /content/datasets/train/RainTrainL/rainstreak-99.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download test model"
      ],
      "metadata": {
        "id": "-_jiD0Puy4P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 197R9YklyJbWobDHQduUFUU2_I0aOxTu1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glouy-YfxUqf",
        "outputId": "2ec76d3e-d115-4265-bec3-79019d048517"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=197R9YklyJbWobDHQduUFUU2_I0aOxTu1\n",
            "To: /content/logs.zip\n",
            "100% 785M/785M [00:10<00:00, 75.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/logs.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv_wdLChxYb_",
        "outputId": "5276ab8c-768f-47e9-d58d-467e57a7a1ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/logs.zip\n",
            "   creating: /content/logs/\n",
            "   creating: /content/logs/Rain100L/\n",
            "   creating: /content/logs/Rain100L/IteDNet/\n",
            "  inflating: /content/logs/Rain100L/IteDNet/events.out.tfevents.1702571352.fptlab  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/log.csv  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/events.out.tfevents.1702571409.fptlab  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/events.out.tfevents.1702572433.fptlab  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_latest.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch1.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch2.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch3.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch4.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch5.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch6.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch7.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch8.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch9.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch10.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch11.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch12.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch13.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch14.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch15.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch16.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch17.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch18.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch19.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch20.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch21.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch22.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch23.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch24.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch25.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch26.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch27.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch28.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch29.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch30.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch31.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch32.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch33.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch34.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch35.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch36.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch37.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch38.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch39.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch40.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch41.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch42.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch43.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch44.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch45.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch46.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch47.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch48.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch49.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch50.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch51.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch52.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch53.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch54.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch55.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch56.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch57.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch58.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch59.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch60.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch61.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch62.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch63.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch64.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch65.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch66.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch67.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch68.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch69.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch70.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch71.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch72.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch73.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch74.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch75.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch76.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch77.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch78.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch79.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch80.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch81.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch82.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch83.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch84.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch85.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch86.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch87.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch88.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch89.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch90.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch91.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch92.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch93.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch94.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch95.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch96.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch97.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch98.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch99.pth  \n",
            "  inflating: /content/logs/Rain100L/IteDNet/net_epoch100.pth  \n",
            "   creating: /content/logs/Rain100L/IReDNet_LSTM/\n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/events.out.tfevents.1702624688.fptlab  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/log.csv  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_latest.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch1.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch2.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch3.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch4.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch5.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch6.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch7.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch8.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch9.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch10.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch11.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch12.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch13.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch14.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch15.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch16.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch17.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch18.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch19.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch20.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch21.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch22.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch23.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch24.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch25.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch26.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch27.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch28.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch29.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch30.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch31.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch32.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch33.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch34.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch35.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch36.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch37.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch38.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch39.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch40.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch41.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch42.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch43.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch44.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch45.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch46.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch47.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch48.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch49.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch50.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch51.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch52.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch53.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch54.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch55.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch56.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch57.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch58.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch59.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch60.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch61.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch62.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch63.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch64.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch65.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch66.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch67.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch68.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch69.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch70.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch71.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch72.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch73.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch74.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch75.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch76.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch77.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch78.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch79.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch80.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch81.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch82.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch83.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch84.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch85.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch86.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch87.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch88.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch89.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch90.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch91.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch92.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch93.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch94.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch95.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch96.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch97.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch98.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch99.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_LSTM/net_epoch100.pth  \n",
            "   creating: /content/logs/Rain100L/IReDNet_GRU/\n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/events.out.tfevents.1702749814.fptlab  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/log.csv  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_latest.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch1.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch2.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch3.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch4.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch5.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch6.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch7.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch8.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch9.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch10.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch11.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch12.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch13.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch14.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch15.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch16.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch17.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch18.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch19.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch20.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch21.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch22.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch23.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch24.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch25.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch26.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch27.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch28.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch29.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch30.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch31.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch32.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch33.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch34.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch35.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch36.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch37.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch38.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch39.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch40.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch41.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch42.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch43.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch44.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch45.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch46.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch47.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch48.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch49.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch50.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch51.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch52.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch53.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch54.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch55.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch56.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch57.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch58.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch59.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch60.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch61.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch62.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch63.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch64.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch65.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch66.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch67.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch68.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch69.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch70.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch71.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch72.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch73.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch74.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch75.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch76.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch77.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch78.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch79.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch80.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch81.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch82.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch83.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch84.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch85.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch86.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch87.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch88.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch89.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch90.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch91.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch92.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch93.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch94.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch95.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch96.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch97.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch98.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch99.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_GRU/net_epoch100.pth  \n",
            "   creating: /content/logs/Rain100L/IReDNet_BiRNN/\n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/events.out.tfevents.1702812177.fptlab  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/log.csv  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_latest.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch1.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch2.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch3.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch4.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch5.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch6.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch7.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch8.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch9.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch10.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch11.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch12.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch13.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch14.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch15.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch16.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch17.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch18.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch19.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch20.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch21.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch22.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch23.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch24.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch25.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch26.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch27.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch28.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch29.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch30.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch31.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch32.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch33.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch34.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch35.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch36.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch37.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch38.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch39.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch40.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch41.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch42.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch43.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch47.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch48.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch49.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch44.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch50.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch45.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch46.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch51.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch52.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch53.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch54.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch55.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch56.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch57.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch58.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch59.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch60.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch61.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch62.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch63.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch64.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch65.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch66.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch67.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch68.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch69.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch70.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch71.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch72.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch73.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch74.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch75.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch76.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch77.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch78.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch79.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch80.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch81.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch82.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch83.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch84.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch85.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch86.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch87.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch88.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch89.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch90.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch91.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch92.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch93.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch94.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch95.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch96.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch97.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch98.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch99.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_BiRNN/net_epoch100.pth  \n",
            "   creating: /content/logs/Rain100L/IReDNet_IndRNN/\n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/events.out.tfevents.1702870824.fptlab  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/log.csv  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_latest.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch1.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch2.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch3.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch4.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch5.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch6.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch7.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch8.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch9.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch10.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch11.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch12.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch13.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch14.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch15.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch16.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch17.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch18.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch19.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch20.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch21.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch22.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch23.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch24.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch25.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch26.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch27.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch28.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch29.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch30.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch31.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch32.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch33.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch34.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch35.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch36.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch37.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch38.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch39.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch40.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch41.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch42.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch43.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch44.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch45.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch46.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch47.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch48.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch49.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch50.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch51.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch52.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch53.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch54.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch55.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch56.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/events.out.tfevents.1702906164.fptlab  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch57.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch58.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch59.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch60.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch61.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch62.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch63.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch64.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch65.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch66.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch67.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch68.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch69.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch70.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch71.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch72.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch73.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch74.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch75.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch76.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch77.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch78.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch79.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch80.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch81.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch82.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch83.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch84.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch85.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch86.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch87.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch88.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch89.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch90.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch91.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch92.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch93.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch94.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch95.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch96.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch97.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch98.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch99.pth  \n",
            "  inflating: /content/logs/Rain100L/IReDNet_IndRNN/net_epoch100.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Model"
      ],
      "metadata": {
        "id": "bWNrSuZD6kQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A3qIG1064Uz",
        "outputId": "5fc55e60-a85d-476b-ef5b-4ec841f4bb38"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.utils as utils\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import csv\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description=\"Proposed1_train\")\n",
        "# parser.add_argument(\"--network\", type=str, default=\"IReDNet\", help='name of network')\n",
        "# parser.add_argument(\"--loss\", type=str, default=\"SSIM\", help='loss function')\n",
        "# parser.add_argument(\"--preprocess\", type=bool, default=True, help='run prepare_data or not')\n",
        "# parser.add_argument(\"--batch_size\", type=int, default=18, help=\"Training batch size\")\n",
        "# parser.add_argument(\"--epochs\", type=int, default=100, help=\"Number of training epochs\")\n",
        "# parser.add_argument(\"--milestone\", type=int, default=[30,50,80], help=\"When to decay learning rate\", nargs='+')\n",
        "# parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"initial learning rate\")\n",
        "# parser.add_argument(\"--save_path\", type=str, default=\"logs/Proposed1_test\", help='path to save models and log files')\n",
        "# parser.add_argument(\"--save_freq\",type=int,default=1,help='save intermediate model')\n",
        "# parser.add_argument(\"--data_path\",type=str, default=\"datasets/train/Rain12600\",help='path to training data')\n",
        "# parser.add_argument(\"--use_gpu\", type=bool, default=True, help='use GPU or not')\n",
        "# parser.add_argument(\"--gpu_id\", type=str, default=\"0\", help='GPU id')\n",
        "# parser.add_argument(\"--recurrent_iter\", type=int, default=6, help='number of recursive stages')\n",
        "# parser.add_argument(\"--optimizer\", type=str, default=\"CustomAdam\", help='Optimizer Adam/SGD/RMSProp/CustomAdam')\n",
        "\n",
        "# opt = parser.parse_args()\n",
        "\n",
        "if use_gpu:\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if preprocess:\n",
        "        if data_path.find('RainTrainH') != -1:\n",
        "            print(data_path.find('RainTrainH'))\n",
        "            prepare_data_RainTrainH(data_path=data_path, patch_size=100, stride=80)\n",
        "        elif data_path.find('RainTrainL') != -1:\n",
        "            prepare_data_RainTrainL(data_path=data_path, patch_size=100, stride=80)\n",
        "        elif data_path.find('Rain12600') != -1:\n",
        "            prepare_data_Rain12600(data_path=data_path, patch_size=100, stride=100)\n",
        "        else:\n",
        "            print('unkown datasets: please define prepare data function in DerainDataset.py')\n",
        "    print(\"Network:\", network)\n",
        "    print(\"Loss:\", loss)\n",
        "    print(\"Optimizer:\", optimizer)\n",
        "    print(\"Recurrent iter:\", recurrent_iter)\n",
        "    print(\"Batch size:\", batch_size)\n",
        "    print(\"Number of epochs:\", epochs)\n",
        "    print(\"Learning rate decay milestone:\", milestone)\n",
        "    print(\"Learning rate:\", lr)\n",
        "    print(\"Log save path:\", save_path)\n",
        "    print(\"Dath path:\", data_path)\n",
        "    print('Loading dataset ...\\n')\n",
        "    dataset_train = Dataset(data_path=data_path)\n",
        "    loader_train = DataLoader(dataset=dataset_train, num_workers=4, batch_size=batch_size, shuffle=True)\n",
        "    print(\"# of training samples: %d\\n\" % int(len(dataset_train)))\n",
        "\n",
        "    # Build model\n",
        "    if (network == \"IteDNet\"):\n",
        "        model = IteDNet(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet\"):\n",
        "        model = IReDNet(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_LSTM\"):\n",
        "        model = IReDNet_LSTM(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_GRU\"):\n",
        "        model = IReDNet_GRU(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_BiRNN\"):\n",
        "        model = IReDNet_BiRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_IndRNN\"):\n",
        "        model = IReDNet_IndRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_ConvLSTM\"):\n",
        "        model = IReDNet_ConvLSTM(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_QRNN\"):\n",
        "        model = IReDNet_QRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "\n",
        "    elif (network == \"LightIteDNet\"):\n",
        "        model = LightIteDNet(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet\"):\n",
        "        model = LightIReDNet(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_LSTM\"):\n",
        "        model = LightIReDNet_LSTM(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_GRU\"):\n",
        "        model = LightIReDNet_GRU(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_BiRNN\"):\n",
        "        model = LightIReDNet_BiRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_IndRNN\"):\n",
        "        model = LightIReDNet_IndRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_ConvLSTM\"):\n",
        "        model = LightIReDNet_ConvLSTM(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_QRNN\"):\n",
        "        model = LightIReDNet_QRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    else:\n",
        "        raise Exception(\"Invalid network name.\")\n",
        "\n",
        "\n",
        "    print_network(model)\n",
        "\n",
        "    # loss function\n",
        "    if (loss == \"MSE\"):\n",
        "        criterion = nn.MSELoss(size_average=False)\n",
        "    else:\n",
        "        criterion = SSIM()\n",
        "\n",
        "\n",
        "    # Move to GPU\n",
        "    if use_gpu:\n",
        "        model = model.cuda()\n",
        "        criterion.cuda()\n",
        "\n",
        "    # optimizer\n",
        "    if (optimizer == \"SGD\"):\n",
        "        print(\"Use SGD as optimizer\")\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    elif (optimizer == \"RMSProp\"):\n",
        "        print(\"Use RMSProp as optimizer\")\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "    elif (optimizer == \"Adam\"):\n",
        "        print(\"Use Adam as optimizer\")\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    else:\n",
        "        print(\"Use CustomAdam as optimizer\")\n",
        "        optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    scheduler = MultiStepLR(optimizer, milestones=milestone, gamma=0.2)  # learning rates\n",
        "\n",
        "    # record training\n",
        "    writer = SummaryWriter(save_path)\n",
        "\n",
        "    # load the lastest model\n",
        "    initial_epoch = findLastCheckpoint(save_dir=save_path)\n",
        "    if initial_epoch > 0:\n",
        "        print('resuming by loading epoch %d' % initial_epoch)\n",
        "        model.load_state_dict(torch.load(os.path.join(save_path, 'net_epoch%d.pth' % initial_epoch)))\n",
        "\n",
        "    with open(save_path+\"/log.csv\", 'w', encoding='UTF8') as f:\n",
        "        csv_writer = csv.writer(f)\n",
        "        # write a row to the csv file\n",
        "        header = ['epoch', 'loss', 'pixel_metric', 'PSNR']\n",
        "        csv_writer.writerow(header)\n",
        "\n",
        "        # start training\n",
        "        step = 0\n",
        "        for epoch in range(initial_epoch, epochs):\n",
        "            scheduler.step(epoch)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                print('learning rate %f' % param_group[\"lr\"])\n",
        "\n",
        "            ## epoch training start\n",
        "            for i, (input_train, target_train) in enumerate(loader_train, 0):\n",
        "                model.train()\n",
        "                model.zero_grad()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                input_train, target_train = Variable(input_train), Variable(target_train)\n",
        "\n",
        "                if use_gpu:\n",
        "                    input_train, target_train = input_train.cuda(), target_train.cuda()\n",
        "\n",
        "                out_train, _ = model(input_train)\n",
        "                pixel_metric = criterion(target_train, out_train)\n",
        "                if (loss == \"NegativeSSIM\"):\n",
        "                    loss = -pixel_metric\n",
        "                else:\n",
        "                    loss = pixel_metric\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # training curve\n",
        "                model.eval()\n",
        "                out_train, _ = model(input_train)\n",
        "                out_train = torch.clamp(out_train, 0., 1.)\n",
        "                psnr_train = batch_PSNR(out_train, target_train, 1.)\n",
        "\n",
        "                csv_writer.writerow([epoch+1, loss.item(), pixel_metric.item(), psnr_train])\n",
        "                print(\"[epoch %d][%d/%d] loss: %.4f, pixel_metric: %.4f, PSNR: %.4f\" %\n",
        "                    (epoch+1, i+1, len(loader_train), loss.item(), pixel_metric.item(), psnr_train))\n",
        "\n",
        "                if step % 10 == 0:\n",
        "                    # Log the scalar values\n",
        "                    writer.add_scalar('loss', loss.item(), step)\n",
        "                    writer.add_scalar('PSNR on training data', psnr_train, step)\n",
        "                step += 1\n",
        "            ## epoch training end\n",
        "\n",
        "            # log the images\n",
        "            model.eval()\n",
        "            out_train, _ = model(input_train)\n",
        "            out_train = torch.clamp(out_train, 0., 1.)\n",
        "            im_target = utils.make_grid(target_train.data, nrow=8, normalize=True, scale_each=True)\n",
        "            im_input = utils.make_grid(input_train.data, nrow=8, normalize=True, scale_each=True)\n",
        "            im_derain = utils.make_grid(out_train.data, nrow=8, normalize=True, scale_each=True)\n",
        "            writer.add_image('clean image', im_target, epoch+1)\n",
        "            writer.add_image('rainy image', im_input, epoch+1)\n",
        "            writer.add_image('deraining image', im_derain, epoch+1)\n",
        "\n",
        "            # save model\n",
        "            torch.save(model.state_dict(), os.path.join(save_path, 'net_latest.pth'))\n",
        "            if epoch % save_freq == 0:\n",
        "                torch.save(model.state_dict(), os.path.join(save_path, 'net_epoch%d.pth' % (epoch+1)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cwHT-sCp6kAv",
        "outputId": "a364209f-6d72-4080-9e8c-2dabf7c7bb3f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "process training data\n",
            "target file: rain-1.png # samples: 15\n",
            "target file: rain-1.png # samples: 15\n",
            "target file: rain-2.png # samples: 15\n",
            "target file: rain-2.png # samples: 15\n",
            "target file: rain-3.png # samples: 15\n",
            "target file: rain-3.png # samples: 15\n",
            "target file: rain-4.png # samples: 15\n",
            "target file: rain-4.png # samples: 15\n",
            "target file: rain-5.png # samples: 15\n",
            "target file: rain-5.png # samples: 15\n",
            "target file: rain-6.png # samples: 15\n",
            "target file: rain-6.png # samples: 15\n",
            "target file: rain-7.png # samples: 15\n",
            "target file: rain-7.png # samples: 15\n",
            "target file: rain-8.png # samples: 15\n",
            "target file: rain-8.png # samples: 15\n",
            "target file: rain-9.png # samples: 15\n",
            "target file: rain-9.png # samples: 15\n",
            "target file: rain-10.png # samples: 15\n",
            "target file: rain-10.png # samples: 15\n",
            "target file: rain-11.png # samples: 15\n",
            "target file: rain-11.png # samples: 15\n",
            "target file: rain-12.png # samples: 15\n",
            "target file: rain-12.png # samples: 15\n",
            "target file: rain-13.png # samples: 15\n",
            "target file: rain-13.png # samples: 15\n",
            "target file: rain-14.png # samples: 15\n",
            "target file: rain-14.png # samples: 15\n",
            "target file: rain-15.png # samples: 15\n",
            "target file: rain-15.png # samples: 15\n",
            "target file: rain-16.png # samples: 15\n",
            "target file: rain-16.png # samples: 15\n",
            "target file: rain-17.png # samples: 15\n",
            "target file: rain-17.png # samples: 15\n",
            "target file: rain-18.png # samples: 15\n",
            "target file: rain-18.png # samples: 15\n",
            "target file: rain-19.png # samples: 15\n",
            "target file: rain-19.png # samples: 15\n",
            "target file: rain-20.png # samples: 15\n",
            "target file: rain-20.png # samples: 15\n",
            "target file: rain-21.png # samples: 15\n",
            "target file: rain-21.png # samples: 15\n",
            "target file: rain-22.png # samples: 15\n",
            "target file: rain-22.png # samples: 15\n",
            "target file: rain-23.png # samples: 15\n",
            "target file: rain-23.png # samples: 15\n",
            "target file: rain-24.png # samples: 15\n",
            "target file: rain-24.png # samples: 15\n",
            "target file: rain-25.png # samples: 15\n",
            "target file: rain-25.png # samples: 15\n",
            "target file: rain-26.png # samples: 15\n",
            "target file: rain-26.png # samples: 15\n",
            "target file: rain-27.png # samples: 15\n",
            "target file: rain-27.png # samples: 15\n",
            "target file: rain-28.png # samples: 15\n",
            "target file: rain-28.png # samples: 15\n",
            "target file: rain-29.png # samples: 15\n",
            "target file: rain-29.png # samples: 15\n",
            "target file: rain-30.png # samples: 15\n",
            "target file: rain-30.png # samples: 15\n",
            "target file: rain-31.png # samples: 15\n",
            "target file: rain-31.png # samples: 15\n",
            "target file: rain-32.png # samples: 15\n",
            "target file: rain-32.png # samples: 15\n",
            "target file: rain-33.png # samples: 15\n",
            "target file: rain-33.png # samples: 15\n",
            "target file: rain-34.png # samples: 15\n",
            "target file: rain-34.png # samples: 15\n",
            "target file: rain-35.png # samples: 15\n",
            "target file: rain-35.png # samples: 15\n",
            "target file: rain-36.png # samples: 15\n",
            "target file: rain-36.png # samples: 15\n",
            "target file: rain-37.png # samples: 15\n",
            "target file: rain-37.png # samples: 15\n",
            "target file: rain-38.png # samples: 15\n",
            "target file: rain-38.png # samples: 15\n",
            "target file: rain-39.png # samples: 15\n",
            "target file: rain-39.png # samples: 15\n",
            "target file: rain-40.png # samples: 15\n",
            "target file: rain-40.png # samples: 15\n",
            "target file: rain-41.png # samples: 15\n",
            "target file: rain-41.png # samples: 15\n",
            "target file: rain-42.png # samples: 15\n",
            "target file: rain-42.png # samples: 15\n",
            "target file: rain-43.png # samples: 15\n",
            "target file: rain-43.png # samples: 15\n",
            "target file: rain-44.png # samples: 15\n",
            "target file: rain-44.png # samples: 15\n",
            "target file: rain-45.png # samples: 15\n",
            "target file: rain-45.png # samples: 15\n",
            "target file: rain-46.png # samples: 15\n",
            "target file: rain-46.png # samples: 15\n",
            "target file: rain-47.png # samples: 15\n",
            "target file: rain-47.png # samples: 15\n",
            "target file: rain-48.png # samples: 15\n",
            "target file: rain-48.png # samples: 15\n",
            "target file: rain-49.png # samples: 15\n",
            "target file: rain-49.png # samples: 15\n",
            "target file: rain-50.png # samples: 15\n",
            "target file: rain-50.png # samples: 15\n",
            "target file: rain-51.png # samples: 15\n",
            "target file: rain-51.png # samples: 15\n",
            "target file: rain-52.png # samples: 15\n",
            "target file: rain-52.png # samples: 15\n",
            "target file: rain-53.png # samples: 15\n",
            "target file: rain-53.png # samples: 15\n",
            "target file: rain-54.png # samples: 15\n",
            "target file: rain-54.png # samples: 15\n",
            "target file: rain-55.png # samples: 15\n",
            "target file: rain-55.png # samples: 15\n",
            "target file: rain-56.png # samples: 15\n",
            "target file: rain-56.png # samples: 15\n",
            "target file: rain-57.png # samples: 15\n",
            "target file: rain-57.png # samples: 15\n",
            "target file: rain-58.png # samples: 15\n",
            "target file: rain-58.png # samples: 15\n",
            "target file: rain-59.png # samples: 15\n",
            "target file: rain-59.png # samples: 15\n",
            "target file: rain-60.png # samples: 15\n",
            "target file: rain-60.png # samples: 15\n",
            "target file: rain-61.png # samples: 15\n",
            "target file: rain-61.png # samples: 15\n",
            "target file: rain-62.png # samples: 15\n",
            "target file: rain-62.png # samples: 15\n",
            "target file: rain-63.png # samples: 15\n",
            "target file: rain-63.png # samples: 15\n",
            "target file: rain-64.png # samples: 15\n",
            "target file: rain-64.png # samples: 15\n",
            "target file: rain-65.png # samples: 15\n",
            "target file: rain-65.png # samples: 15\n",
            "target file: rain-66.png # samples: 15\n",
            "target file: rain-66.png # samples: 15\n",
            "target file: rain-67.png # samples: 15\n",
            "target file: rain-67.png # samples: 15\n",
            "target file: rain-68.png # samples: 15\n",
            "target file: rain-68.png # samples: 15\n",
            "target file: rain-69.png # samples: 15\n",
            "target file: rain-69.png # samples: 15\n",
            "target file: rain-70.png # samples: 15\n",
            "target file: rain-70.png # samples: 15\n",
            "target file: rain-71.png # samples: 15\n",
            "target file: rain-71.png # samples: 15\n",
            "target file: rain-72.png # samples: 15\n",
            "target file: rain-72.png # samples: 15\n",
            "target file: rain-73.png # samples: 15\n",
            "target file: rain-73.png # samples: 15\n",
            "target file: rain-74.png # samples: 15\n",
            "target file: rain-74.png # samples: 15\n",
            "target file: rain-75.png # samples: 15\n",
            "target file: rain-75.png # samples: 15\n",
            "target file: rain-76.png # samples: 15\n",
            "target file: rain-76.png # samples: 15\n",
            "target file: rain-77.png # samples: 15\n",
            "target file: rain-77.png # samples: 15\n",
            "target file: rain-78.png # samples: 15\n",
            "target file: rain-78.png # samples: 15\n",
            "target file: rain-79.png # samples: 15\n",
            "target file: rain-79.png # samples: 15\n",
            "target file: rain-80.png # samples: 15\n",
            "target file: rain-80.png # samples: 15\n",
            "target file: rain-81.png # samples: 15\n",
            "target file: rain-81.png # samples: 15\n",
            "target file: rain-82.png # samples: 15\n",
            "target file: rain-82.png # samples: 15\n",
            "target file: rain-83.png # samples: 15\n",
            "target file: rain-83.png # samples: 15\n",
            "target file: rain-84.png # samples: 15\n",
            "target file: rain-84.png # samples: 15\n",
            "target file: rain-85.png # samples: 15\n",
            "target file: rain-85.png # samples: 15\n",
            "target file: rain-86.png # samples: 15\n",
            "target file: rain-86.png # samples: 15\n",
            "target file: rain-87.png # samples: 15\n",
            "target file: rain-87.png # samples: 15\n",
            "target file: rain-88.png # samples: 15\n",
            "target file: rain-88.png # samples: 15\n",
            "target file: rain-89.png # samples: 15\n",
            "target file: rain-89.png # samples: 15\n",
            "target file: rain-90.png # samples: 15\n",
            "target file: rain-90.png # samples: 15\n",
            "target file: rain-91.png # samples: 15\n",
            "target file: rain-91.png # samples: 15\n",
            "target file: rain-92.png # samples: 15\n",
            "target file: rain-92.png # samples: 15\n",
            "target file: rain-93.png # samples: 15\n",
            "target file: rain-93.png # samples: 15\n",
            "target file: rain-94.png # samples: 15\n",
            "target file: rain-94.png # samples: 15\n",
            "target file: rain-95.png # samples: 15\n",
            "target file: rain-95.png # samples: 15\n",
            "target file: rain-96.png # samples: 15\n",
            "target file: rain-96.png # samples: 15\n",
            "target file: rain-97.png # samples: 15\n",
            "target file: rain-97.png # samples: 15\n",
            "target file: rain-98.png # samples: 15\n",
            "target file: rain-98.png # samples: 15\n",
            "target file: rain-99.png # samples: 15\n",
            "target file: rain-99.png # samples: 15\n",
            "target file: rain-100.png # samples: 15\n",
            "target file: rain-100.png # samples: 15\n",
            "target file: rain-101.png # samples: 15\n",
            "target file: rain-101.png # samples: 15\n",
            "target file: rain-102.png # samples: 15\n",
            "target file: rain-102.png # samples: 15\n",
            "target file: rain-103.png # samples: 15\n",
            "target file: rain-103.png # samples: 15\n",
            "target file: rain-104.png # samples: 15\n",
            "target file: rain-104.png # samples: 15\n",
            "target file: rain-105.png # samples: 15\n",
            "target file: rain-105.png # samples: 15\n",
            "target file: rain-106.png # samples: 15\n",
            "target file: rain-106.png # samples: 15\n",
            "target file: rain-107.png # samples: 15\n",
            "target file: rain-107.png # samples: 15\n",
            "target file: rain-108.png # samples: 15\n",
            "target file: rain-108.png # samples: 15\n",
            "target file: rain-109.png # samples: 15\n",
            "target file: rain-109.png # samples: 15\n",
            "target file: rain-110.png # samples: 15\n",
            "target file: rain-110.png # samples: 15\n",
            "target file: rain-111.png # samples: 15\n",
            "target file: rain-111.png # samples: 15\n",
            "target file: rain-112.png # samples: 15\n",
            "target file: rain-112.png # samples: 15\n",
            "target file: rain-113.png # samples: 15\n",
            "target file: rain-113.png # samples: 15\n",
            "target file: rain-114.png # samples: 15\n",
            "target file: rain-114.png # samples: 15\n",
            "target file: rain-115.png # samples: 15\n",
            "target file: rain-115.png # samples: 15\n",
            "target file: rain-116.png # samples: 15\n",
            "target file: rain-116.png # samples: 15\n",
            "target file: rain-117.png # samples: 15\n",
            "target file: rain-117.png # samples: 15\n",
            "target file: rain-118.png # samples: 15\n",
            "target file: rain-118.png # samples: 15\n",
            "target file: rain-119.png # samples: 15\n",
            "target file: rain-119.png # samples: 15\n",
            "target file: rain-120.png # samples: 15\n",
            "target file: rain-120.png # samples: 15\n",
            "target file: rain-121.png # samples: 15\n",
            "target file: rain-121.png # samples: 15\n",
            "target file: rain-122.png # samples: 15\n",
            "target file: rain-122.png # samples: 15\n",
            "target file: rain-123.png # samples: 15\n",
            "target file: rain-123.png # samples: 15\n",
            "target file: rain-124.png # samples: 15\n",
            "target file: rain-124.png # samples: 15\n",
            "target file: rain-125.png # samples: 15\n",
            "target file: rain-125.png # samples: 15\n",
            "target file: rain-126.png # samples: 15\n",
            "target file: rain-126.png # samples: 15\n",
            "target file: rain-127.png # samples: 15\n",
            "target file: rain-127.png # samples: 15\n",
            "target file: rain-128.png # samples: 15\n",
            "target file: rain-128.png # samples: 15\n",
            "target file: rain-129.png # samples: 15\n",
            "target file: rain-129.png # samples: 15\n",
            "target file: rain-130.png # samples: 15\n",
            "target file: rain-130.png # samples: 15\n",
            "target file: rain-131.png # samples: 15\n",
            "target file: rain-131.png # samples: 15\n",
            "target file: rain-132.png # samples: 15\n",
            "target file: rain-132.png # samples: 15\n",
            "target file: rain-133.png # samples: 15\n",
            "target file: rain-133.png # samples: 15\n",
            "target file: rain-134.png # samples: 15\n",
            "target file: rain-134.png # samples: 15\n",
            "target file: rain-135.png # samples: 15\n",
            "target file: rain-135.png # samples: 15\n",
            "target file: rain-136.png # samples: 15\n",
            "target file: rain-136.png # samples: 15\n",
            "target file: rain-137.png # samples: 15\n",
            "target file: rain-137.png # samples: 15\n",
            "target file: rain-138.png # samples: 15\n",
            "target file: rain-138.png # samples: 15\n",
            "target file: rain-139.png # samples: 15\n",
            "target file: rain-139.png # samples: 15\n",
            "target file: rain-140.png # samples: 15\n",
            "target file: rain-140.png # samples: 15\n",
            "target file: rain-141.png # samples: 15\n",
            "target file: rain-141.png # samples: 15\n",
            "target file: rain-142.png # samples: 15\n",
            "target file: rain-142.png # samples: 15\n",
            "target file: rain-143.png # samples: 15\n",
            "target file: rain-143.png # samples: 15\n",
            "target file: rain-144.png # samples: 15\n",
            "target file: rain-144.png # samples: 15\n",
            "target file: rain-145.png # samples: 15\n",
            "target file: rain-145.png # samples: 15\n",
            "target file: rain-146.png # samples: 15\n",
            "target file: rain-146.png # samples: 15\n",
            "target file: rain-147.png # samples: 15\n",
            "target file: rain-147.png # samples: 15\n",
            "target file: rain-148.png # samples: 15\n",
            "target file: rain-148.png # samples: 15\n",
            "target file: rain-149.png # samples: 15\n",
            "target file: rain-149.png # samples: 15\n",
            "target file: rain-150.png # samples: 15\n",
            "target file: rain-150.png # samples: 15\n",
            "target file: rain-151.png # samples: 15\n",
            "target file: rain-151.png # samples: 15\n",
            "target file: rain-152.png # samples: 15\n",
            "target file: rain-152.png # samples: 15\n",
            "target file: rain-153.png # samples: 15\n",
            "target file: rain-153.png # samples: 15\n",
            "target file: rain-154.png # samples: 15\n",
            "target file: rain-154.png # samples: 15\n",
            "target file: rain-155.png # samples: 15\n",
            "target file: rain-155.png # samples: 15\n",
            "target file: rain-156.png # samples: 15\n",
            "target file: rain-156.png # samples: 15\n",
            "target file: rain-157.png # samples: 15\n",
            "target file: rain-157.png # samples: 15\n",
            "target file: rain-158.png # samples: 15\n",
            "target file: rain-158.png # samples: 15\n",
            "target file: rain-159.png # samples: 15\n",
            "target file: rain-159.png # samples: 15\n",
            "target file: rain-160.png # samples: 15\n",
            "target file: rain-160.png # samples: 15\n",
            "target file: rain-161.png # samples: 15\n",
            "target file: rain-161.png # samples: 15\n",
            "target file: rain-162.png # samples: 15\n",
            "target file: rain-162.png # samples: 15\n",
            "target file: rain-163.png # samples: 15\n",
            "target file: rain-163.png # samples: 15\n",
            "target file: rain-164.png # samples: 15\n",
            "target file: rain-164.png # samples: 15\n",
            "target file: rain-165.png # samples: 15\n",
            "target file: rain-165.png # samples: 15\n",
            "target file: rain-166.png # samples: 15\n",
            "target file: rain-166.png # samples: 15\n",
            "target file: rain-167.png # samples: 15\n",
            "target file: rain-167.png # samples: 15\n",
            "target file: rain-168.png # samples: 15\n",
            "target file: rain-168.png # samples: 15\n",
            "target file: rain-169.png # samples: 15\n",
            "target file: rain-169.png # samples: 15\n",
            "target file: rain-170.png # samples: 15\n",
            "target file: rain-170.png # samples: 15\n",
            "target file: rain-171.png # samples: 15\n",
            "target file: rain-171.png # samples: 15\n",
            "target file: rain-172.png # samples: 15\n",
            "target file: rain-172.png # samples: 15\n",
            "target file: rain-173.png # samples: 15\n",
            "target file: rain-173.png # samples: 15\n",
            "target file: rain-174.png # samples: 15\n",
            "target file: rain-174.png # samples: 15\n",
            "target file: rain-175.png # samples: 15\n",
            "target file: rain-175.png # samples: 15\n",
            "target file: rain-176.png # samples: 15\n",
            "target file: rain-176.png # samples: 15\n",
            "target file: rain-177.png # samples: 15\n",
            "target file: rain-177.png # samples: 15\n",
            "target file: rain-178.png # samples: 15\n",
            "target file: rain-178.png # samples: 15\n",
            "target file: rain-179.png # samples: 15\n",
            "target file: rain-179.png # samples: 15\n",
            "target file: rain-180.png # samples: 15\n",
            "target file: rain-180.png # samples: 15\n",
            "target file: rain-181.png # samples: 15\n",
            "target file: rain-181.png # samples: 15\n",
            "target file: rain-182.png # samples: 15\n",
            "target file: rain-182.png # samples: 15\n",
            "target file: rain-183.png # samples: 15\n",
            "target file: rain-183.png # samples: 15\n",
            "target file: rain-184.png # samples: 15\n",
            "target file: rain-184.png # samples: 15\n",
            "target file: rain-185.png # samples: 15\n",
            "target file: rain-185.png # samples: 15\n",
            "target file: rain-186.png # samples: 15\n",
            "target file: rain-186.png # samples: 15\n",
            "target file: rain-187.png # samples: 15\n",
            "target file: rain-187.png # samples: 15\n",
            "target file: rain-188.png # samples: 15\n",
            "target file: rain-188.png # samples: 15\n",
            "target file: rain-189.png # samples: 15\n",
            "target file: rain-189.png # samples: 15\n",
            "target file: rain-190.png # samples: 15\n",
            "target file: rain-190.png # samples: 15\n",
            "target file: rain-191.png # samples: 15\n",
            "target file: rain-191.png # samples: 15\n",
            "target file: rain-192.png # samples: 15\n",
            "target file: rain-192.png # samples: 15\n",
            "target file: rain-193.png # samples: 15\n",
            "target file: rain-193.png # samples: 15\n",
            "target file: rain-194.png # samples: 15\n",
            "target file: rain-194.png # samples: 15\n",
            "target file: rain-195.png # samples: 15\n",
            "target file: rain-195.png # samples: 15\n",
            "target file: rain-196.png # samples: 15\n",
            "target file: rain-196.png # samples: 15\n",
            "target file: rain-197.png # samples: 15\n",
            "target file: rain-197.png # samples: 15\n",
            "target file: rain-198.png # samples: 15\n",
            "target file: rain-198.png # samples: 15\n",
            "target file: rain-199.png # samples: 15\n",
            "target file: rain-199.png # samples: 15\n",
            "target file: rain-200.png # samples: 15\n",
            "target file: rain-200.png # samples: 15\n",
            "training set, # samples 6000\n",
            "\n",
            "Network: IReDNet\n",
            "Loss: NegativeSSIM\n",
            "Optimizer: Adam\n",
            "Recurrent iter: 6\n",
            "Batch size: 2\n",
            "Number of epochs: 100\n",
            "Learning rate decay milestone: [30, 50, 80]\n",
            "Learning rate: 0.001\n",
            "Log save path: /content/logs/Rain100L/IReDNet_NegativeSSIM\n",
            "Dath path: /content/datasets/train/RainTrainL\n",
            "Loading dataset ...\n",
            "\n",
            "# of training samples: 6000\n",
            "\n",
            "IReDNet(\n",
            "  (conv0): Sequential(\n",
            "    (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (conv_i): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (conv_f): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (conv_g): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Tanh()\n",
            "  )\n",
            "  (conv_o): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (dense_block): Sequential(\n",
            "    (0): Dense_Block(\n",
            "      (block): ModuleDict(\n",
            "        (dense0): Dense_Layer(\n",
            "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (dense1): Dense_Layer(\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (dense2): Dense_Layer(\n",
            "          (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (dense3): Dense_Layer(\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (dense4): Dense_Layer(\n",
            "          (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (dense5): Dense_Layer(\n",
            "          (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(224, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "Total number of parameters: 391747\n",
            "Use Adam as optimizer\n",
            "learning rate 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "<ipython-input-5-03b830130c65>:29: UserWarning: DEPRECATED: skimage.measure.compare_psnr has been moved to skimage.metrics.peak_signal_noise_ratio. It will be removed from skimage.measure in version 0.18.\n",
            "  PSNR += compare_psnr(Iclean[i,:,:,:], Img[i,:,:,:], data_range=data_range)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 1][1/3000] loss: -0.8728, pixel_metric: 0.8728, PSNR: 27.1556\n",
            "[epoch 1][2/3000] loss: 0.8512, pixel_metric: 0.8512, PSNR: 20.7490\n",
            "[epoch 1][3/3000] loss: 0.7737, pixel_metric: 0.7737, PSNR: 19.1777\n",
            "[epoch 1][4/3000] loss: 0.7891, pixel_metric: 0.7891, PSNR: 10.3370\n",
            "[epoch 1][5/3000] loss: 0.5348, pixel_metric: 0.5348, PSNR: 5.0118\n",
            "[epoch 1][6/3000] loss: -0.3039, pixel_metric: -0.3039, PSNR: 4.8200\n",
            "[epoch 1][7/3000] loss: -0.0363, pixel_metric: -0.0363, PSNR: 3.0722\n",
            "[epoch 1][8/3000] loss: -0.0288, pixel_metric: -0.0288, PSNR: 4.9675\n",
            "[epoch 1][9/3000] loss: -0.0245, pixel_metric: -0.0245, PSNR: 3.9817\n",
            "[epoch 1][10/3000] loss: -0.0093, pixel_metric: -0.0093, PSNR: 5.1431\n",
            "[epoch 1][11/3000] loss: -0.0140, pixel_metric: -0.0140, PSNR: 4.9017\n",
            "[epoch 1][12/3000] loss: -0.0129, pixel_metric: -0.0129, PSNR: 4.1853\n",
            "[epoch 1][13/3000] loss: -0.0130, pixel_metric: -0.0130, PSNR: 4.5276\n",
            "[epoch 1][14/3000] loss: -0.0158, pixel_metric: -0.0158, PSNR: 3.8154\n",
            "[epoch 1][15/3000] loss: -0.0132, pixel_metric: -0.0132, PSNR: 5.3669\n",
            "[epoch 1][16/3000] loss: -0.0108, pixel_metric: -0.0108, PSNR: 5.0840\n",
            "[epoch 1][17/3000] loss: -0.0048, pixel_metric: -0.0048, PSNR: 7.7685\n",
            "[epoch 1][18/3000] loss: -0.0111, pixel_metric: -0.0111, PSNR: 5.0725\n",
            "[epoch 1][19/3000] loss: -0.0115, pixel_metric: -0.0115, PSNR: 5.0285\n",
            "[epoch 1][20/3000] loss: -0.0152, pixel_metric: -0.0152, PSNR: 5.0795\n",
            "[epoch 1][21/3000] loss: -0.0022, pixel_metric: -0.0022, PSNR: 2.8734\n",
            "[epoch 1][22/3000] loss: -0.0164, pixel_metric: -0.0164, PSNR: 4.7668\n",
            "[epoch 1][23/3000] loss: -0.0267, pixel_metric: -0.0267, PSNR: 4.5567\n",
            "[epoch 1][24/3000] loss: -0.0176, pixel_metric: -0.0176, PSNR: 5.7909\n",
            "[epoch 1][25/3000] loss: -0.0216, pixel_metric: -0.0216, PSNR: 5.2416\n",
            "[epoch 1][26/3000] loss: -0.0520, pixel_metric: -0.0520, PSNR: 3.8635\n",
            "[epoch 1][27/3000] loss: -0.0276, pixel_metric: -0.0276, PSNR: 6.0279\n",
            "[epoch 1][28/3000] loss: -0.0481, pixel_metric: -0.0481, PSNR: 5.0983\n",
            "[epoch 1][29/3000] loss: -0.0610, pixel_metric: -0.0610, PSNR: 4.5756\n",
            "[epoch 1][30/3000] loss: -0.1103, pixel_metric: -0.1103, PSNR: 3.4924\n",
            "[epoch 1][31/3000] loss: -0.0530, pixel_metric: -0.0530, PSNR: 5.6287\n",
            "[epoch 1][32/3000] loss: -0.0867, pixel_metric: -0.0867, PSNR: 4.7197\n",
            "[epoch 1][33/3000] loss: -0.1073, pixel_metric: -0.1073, PSNR: 4.8442\n",
            "[epoch 1][34/3000] loss: -0.1424, pixel_metric: -0.1424, PSNR: 5.1698\n",
            "[epoch 1][35/3000] loss: -0.1770, pixel_metric: -0.1770, PSNR: 5.2419\n",
            "[epoch 1][36/3000] loss: -0.2530, pixel_metric: -0.2530, PSNR: 6.6742\n",
            "[epoch 1][37/3000] loss: 0.1539, pixel_metric: 0.1539, PSNR: 5.0570\n",
            "[epoch 1][38/3000] loss: -0.0308, pixel_metric: -0.0308, PSNR: 4.0770\n",
            "[epoch 1][39/3000] loss: 0.2201, pixel_metric: 0.2201, PSNR: 3.3545\n",
            "[epoch 1][40/3000] loss: 0.0713, pixel_metric: 0.0713, PSNR: 4.7548\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3eeeec7170f2>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0mout_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0mpixel_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NegativeSSIM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-66797cfea8f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-66797cfea8f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-66797cfea8f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, prev_features)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Model"
      ],
      "metadata": {
        "id": "cDtweWnFy1zW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "AgO5Hc0lYWz9",
        "outputId": "5652c281-944c-486b-a4b7-226ab55fe3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model ...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-10992c2f5148>\u001b[0m in \u001b[0;36m<cell line: 123>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-10992c2f5148>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'net_latest.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \"\"\"\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \"\"\"\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import argparse\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "\n",
        "# parser = argparse.ArgumentParser(description=\"IReDNet_Test\")\n",
        "# parser.add_argument(\"--network\", type=str, default=\"IReDNet\", help='name of network')\n",
        "# parser.add_argument(\"--logdir\", type=str, default=\"logs/IReDNet/\", help='path to model and log files')\n",
        "# parser.add_argument(\"--data_path\", type=str, default=\"datasets/test/Rain100H/rainy_image\", help='path to training data')\n",
        "# parser.add_argument(\"--save_path\", type=str, default=\"results/Rain100H/IReDNet\", help='path to save results')\n",
        "# parser.add_argument(\"--use_gpu\", type=bool, default=True, help='use GPU or not')\n",
        "# parser.add_argument(\"--gpu_id\", type=str, default=\"0\", help='GPU id')\n",
        "# parser.add_argument(\"--recurrent_iter\", type=int, default=6, help='number of recursive stages')\n",
        "# opt = parser.parse_args()\n",
        "\n",
        "if use_gpu:\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
        "\n",
        "\n",
        "def main():\n",
        "    loss = \"NegativeSSIM\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Build model\n",
        "    print('Loading model ...\\n')\n",
        " # Build model\n",
        "    if (network == \"IteDNet\"):\n",
        "        model = IteDNet(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet\"):\n",
        "        model = IReDNet(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_LSTM\"):\n",
        "        model = IReDNet_LSTM(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_GRU\"):\n",
        "        model = IReDNet_GRU(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_BiRNN\"):\n",
        "        model = IReDNet_BiRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_IndRNN\"):\n",
        "        model = IReDNet_IndRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_ConvLSTM\"):\n",
        "        model = IReDNet_ConvLSTM(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"IReDNet_QRNN\"):\n",
        "        model = IReDNet_QRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIteDNet\"):\n",
        "        model = LightIteDNet(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet\"):\n",
        "        model = LightIReDNet(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_LSTM\"):\n",
        "        model = LightIReDNet_LSTM(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_GRU\"):\n",
        "        model = LightIReDNet_GRU(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_BiRNN\"):\n",
        "        model = LightIReDNet_BiRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_IndRNN\"):\n",
        "        model = LightIReDNet_IndRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_ConvLSTM\"):\n",
        "        model = LightIReDNet_ConvLSTM(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    elif (network == \"LightIReDNet_QRNN\"):\n",
        "        model = LightIReDNet_QRNN(recurrent_iter=recurrent_iter, use_GPU=use_gpu)\n",
        "    else:\n",
        "        raise Exception(\"Invalid network name.\")\n",
        "\n",
        "    if use_gpu:\n",
        "        model = model.cuda()\n",
        "\n",
        "    model.load_state_dict(torch.load(os.path.join(logdir, 'net_latest.pth')))\n",
        "    model.eval()\n",
        "\n",
        "    time_test = 0\n",
        "    count = 0\n",
        "    for img_name in os.listdir(data_path):\n",
        "        if is_image(img_name):\n",
        "            img_path = os.path.join(data_path, img_name)\n",
        "\n",
        "            # input image\n",
        "            y = cv2.imread(img_path)\n",
        "            b, g, r = cv2.split(y)\n",
        "            y = cv2.merge([r, g, b])\n",
        "            #y = cv2.resize(y, (int(500), int(500)), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            y = normalize(np.float32(y))\n",
        "            y = np.expand_dims(y.transpose(2, 0, 1), 0)\n",
        "            y = Variable(torch.Tensor(y))\n",
        "\n",
        "            if use_gpu:\n",
        "                y = y.cuda()\n",
        "\n",
        "            with torch.no_grad(): #\n",
        "                if use_gpu:\n",
        "                    torch.cuda.synchronize()\n",
        "                start_time = time.time()\n",
        "\n",
        "                out, _ = model(y)\n",
        "                out = torch.clamp(out, 0., 1.)\n",
        "\n",
        "                if use_gpu:\n",
        "                    torch.cuda.synchronize()\n",
        "                end_time = time.time()\n",
        "                dur_time = end_time - start_time\n",
        "                time_test += dur_time\n",
        "\n",
        "                print(img_name, ': ', dur_time)\n",
        "\n",
        "            if use_gpu:\n",
        "                save_out = np.uint8(255 * out.data.cpu().numpy().squeeze())   #back to cpu\n",
        "            else:\n",
        "                save_out = np.uint8(255 * out.data.numpy().squeeze())\n",
        "\n",
        "            save_out = save_out.transpose(1, 2, 0)\n",
        "            b, g, r = cv2.split(save_out)\n",
        "            save_out = cv2.merge([r, g, b])\n",
        "\n",
        "            cv2.imwrite(os.path.join(save_path, img_name), save_out)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "    print('Avg. time:', time_test/count)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zip results"
      ],
      "metadata": {
        "id": "Oko7juFi1Bwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Open the folder that you want to zip.\n",
        "with zipfile.ZipFile('IReDNet.zip', 'w') as zip_file:\n",
        "  # Iterate over all the files in the folder.\n",
        "  for file in os.listdir('/content/results/Rain100L/IteDNet/'):\n",
        "    # Add each file to the zip file.\n",
        "    zip_file.write(os.path.join('/content/results/Rain100L/IteDNet/', file))\n",
        "  # Iterate over all the files in the folder.\n",
        "  for file in os.listdir('/content/results/Rain100L/IReDNet_LSTM/'):\n",
        "    # Add each file to the zip file.\n",
        "    zip_file.write(os.path.join('/content/results/Rain100L/IReDNet_LSTM/', file))\n",
        "  # Iterate over all the files in the folder.\n",
        "  for file in os.listdir('/content/results/Rain100L/IReDNet_IndRNN/'):\n",
        "    # Add each file to the zip file.\n",
        "    zip_file.write(os.path.join('/content/results/Rain100L/IReDNet_IndRNN/', file))\n",
        "  # Iterate over all the files in the folder.\n",
        "  for file in os.listdir('/content/results/Rain100L/IReDNet_GRU/'):\n",
        "    # Add each file to the zip file.\n",
        "    zip_file.write(os.path.join('/content/results/Rain100L/IReDNet_GRU/', file))\n",
        "  # Iterate over all the files in the folder.\n",
        "  for file in os.listdir('/content/results/Rain100L/IReDNet_BiRNN/'):\n",
        "    # Add each file to the zip file.\n",
        "    zip_file.write(os.path.join('/content/results/Rain100L/IReDNet_BiRNN/', file))"
      ],
      "metadata": {
        "id": "xqQU_Vjn1Bb6"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}